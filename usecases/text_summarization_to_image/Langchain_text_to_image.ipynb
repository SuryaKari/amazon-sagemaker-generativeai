{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUHkdwOjgPwh",
    "tags": []
   },
   "source": [
    "## **Langchain Usecase: Convert a large document text to image using chaining.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhxo_htsgPwj"
   },
   "source": [
    "***\n",
    "\n",
    "This usecase demonstrates the efficient chaining of text-summarization and text-to-image generation foundational models using Langchain. These foundational models are deployed as [sagemaker jumpstart endpoints](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-deploy.html).\n",
    "\n",
    "### **Sequence of Steps.**\n",
    "\n",
    "* Extract text from a blog post URL.\n",
    "* Create a 'data' folder.\n",
    "* Write the extracted text into a 'blogpost.txt' document and store the document in 'data' folder.\n",
    "* Create text chunks of the document.\n",
    "* Implement Langchain ['map_reduce'](https://python.langchain.com/docs/modules/chains/popular/summarize) chain to efficiently extract summaries from the text chunks.\n",
    "* Connect the above blog summary to a text-to-image model, to generate a  visually appealing final image.\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEdDf7bUgPwj"
   },
   "source": [
    "***\n",
    "\n",
    "Note: This notebook was tested on ml.t3.medium instance in Amazon SageMaker Studio with Python 3 (Data Science 3.0) kernel.\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzVa4dSogPwk"
   },
   "source": [
    "### **1. Import all the required libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQCxGfQ8gPwk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade sagemaker\n",
    "!pip install langchain --quiet\n",
    "!pip install sqlalchemy\n",
    "!pip install -U langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7zRVIStgPwl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Install transformers\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g6r7fjh9gPwl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import all the required libraries\n",
    "\n",
    "import langchain\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain_community.llms import SagemakerEndpoint\n",
    "from langchain_community.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "aws_region = boto3.Session().region_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zmaCof0gPwm"
   },
   "source": [
    "### **2. Get Data : Extract text from blog post url**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zsmE4LfDgPwm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new directory named 'data'\n",
    "os.mkdir('data')\n",
    "\n",
    "\n",
    "#Define the url of the blog post\n",
    "target_url = \"https://www.energy.gov/energysaver/using-solar-electricity-home\"\n",
    "\n",
    "#Sending a HTTP GET request to get URL content.\n",
    "response = requests.get(target_url, allow_redirects=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SENrLE2ogPwm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parse the HTML content and find all the text content\n",
    "html_content = response.content\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "web_text = soup.find_all(string=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UnS0Zm_kgPwm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize an empty string to store the final text\n",
    "final_text = ''\n",
    "\n",
    "# List of HTML elements to be ignored during text filtering\n",
    "blacklist = [\n",
    "    '[document]',\n",
    "    'meta',\n",
    "    'noscript',\n",
    "    'header',\n",
    "    'html',\n",
    "    'meta',\n",
    "    'head',\n",
    "    'input',\n",
    "    'script',\n",
    "    'style',\n",
    "    'button',\n",
    "    'link',\n",
    "    'form',\n",
    "    'object',\n",
    "    'script'\n",
    "]\n",
    "\n",
    "# Loop through all the extracted text\n",
    "for text in web_text:\n",
    "    # Check if the parent tag of the text is not in the blacklist\n",
    "    if text.parent.name not in blacklist and text != '\\n':\n",
    "        # Append the text to the final_text string, adding a space between each text segment\n",
    "        final_text += '{} '.format(text)\n",
    "\n",
    "# Write the contents of final_text into 'blogtext.txt'\n",
    "with open('data/blogtext.txt', 'w') as file:\n",
    "    file.write(final_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPLIPToogPwm"
   },
   "source": [
    "### **3. Prepare Data : Create text chunks from input document**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16fysd_QgPwn"
   },
   "source": [
    " ***\n",
    "\n",
    "Using [Langchain RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/) to split the input document text into smaller chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_rJMovmgPwn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create text_splitter for our input text\n",
    "text_splitter = RecursiveCharacterTextSplitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22u0ZaxsgPwn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Read the document text in input_blog\n",
    "with open(\"data/blogtext.txt\") as f:\n",
    "    input_blog = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3DVeXhZgPwn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWNbx-jwgPwn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating document chunks for our input_blog to reduce the computational expense of processing large documents.\n",
    "input_documents = text_splitter.create_documents([input_blog])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSpKp8k9gPwn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nyp3eys6gPwn"
   },
   "source": [
    "### **4. Retrieve the deployed models instances**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the variable `endpoint_name` with the name of the endpoint you deployed for the Falcon 7V model. You can retrieve that by navigating on the left side through Home -> Deployments -> Endpoints. Select the ednpoint that starts with \"hf-llm-falcon-7b-instruct-bf16-\" and replace its value with the variable `endpoint_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the existing endpoint name\n",
    "falcon_endpoint_name = \"<endpoint name>\"  # Replace with your endpoint name\n",
    "\n",
    "# Create a SageMaker predictor object\n",
    "predictor_txt = Predictor(\n",
    "    endpoint_name=falcon_endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")\n",
    "\n",
    "predictor_txt.endpoint_name "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the variable `endpoint_name` with the name of the endpoint you deployed for the Stable Diffusion model. You can retrieve that by navigating on the left side through Home -> Deployments -> Endpoints. Select the ednpoint that starts with \"stabilityai-stable-diffusion-xl-base-1-\" and replace its value with the variable `endpoint_name` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the existing endpoint name\n",
    "sd_endpoint_name = \"<endpoint name>\"  # Replace with your endpoint name\n",
    "\n",
    "# Create a SageMaker predictor object\n",
    "predictor_img = Predictor(\n",
    "    endpoint_name=sd_endpoint_name,\n",
    ")\n",
    "\n",
    "predictor_img.endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWS0OpWjgPwo"
   },
   "source": [
    "### **5. Create a chain for Text Summarization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdGbb_tlgPwo"
   },
   "source": [
    "![SUMMARY CHAIN](images/Summary_chain.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-neytI93gPwo"
   },
   "source": [
    "### **MapReduce Method to summarize large input document text.**\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "* MapReduceDocumentsChain is used For efficient summary generation from chunks of a large input document. MapReduceDocumentsChain can be used as a part of Langchain **load_summarize_chain**.\n",
    "\n",
    "* To implement map reduce approach , define chain_type as [\"map_reduce\"](https://github.com/hwchase17/langchain/blob/6a64870ea05ad6a750b8753ce7477a5355539f0d/langchain/chains/combine_documents/map_reduce.py).\n",
    "\n",
    "* MapReduceDocumentsChain,  uses **map_prompt** on each text chunk to extract their summaries, and uses **combine_prompt** to generate a combined summary of all these generated summaries.\n",
    "\n",
    "* **Prompt Example for map_reduce type of Chain :** [Example Prompt](https://github.com/hwchase17/langchain/blob/master/langchain/chains/summarize/map_reduce_prompt.py). It takes \"text\" as input variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elqK_vv0gPwo"
   },
   "source": [
    "### **Content Handler for Falcon 7B Instruct BF16**\n",
    "\n",
    "***\n",
    "* [A handler class](https://github.com/hwchase17/langchain/blob/6a64870ea05ad6a750b8753ce7477a5355539f0d/langchain/llms/sagemaker_endpoint.py#L64) to transform input from LLM to a format that SageMaker endpoint expects. Similarily,the class also handles transforming output from the SageMaker endpoint to a format that LLM class expects.\n",
    "\n",
    "   \n",
    "* **content_type: Optional[str]** :The MIME type of the input data passed to endpoint\n",
    "* **accepts: Optional[str]** : The MIME type of the response data returned from endpoint\"\n",
    "* **def transform_input(self, prompt: INPUT_TYPE, model_kwargs: Dict) -> bytes:** : Transforms the input to a format that model can accept as the request Body. Should return bytes or seekable file like object in the format specified in the content_type request header.\n",
    "* **def transform_output(self, output: bytes) -> OUTPUT_TYPE:** : Transforms the output from the model to string that the LLM class expects.\n",
    "* **LLMContentHandler(ContentHandlerBase[str, str]):** : Content handler for LLM class.\n",
    "\n",
    "#### **[NOTE : Input Output Reference for Falcon 7B Instruct BF16](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-falcon.ipynb)**\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Ph6EJ7bgPwo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Class that provides input and output transform functions to handle formats between LLM and the endpoint.\n",
    "class ContentHandlerTextSummarization(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> json:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        summary = (response_json[0]['generated_text'])\n",
    "        if(len(summary) <= 1):\n",
    "            print(\"Final Summary : \\n\", summary)\n",
    "            return summary\n",
    "        else:\n",
    "            summary_list = summary.split(\".\")\n",
    "            summary_list.pop()\n",
    "            summary = ' '.join(str(elem) for elem in summary_list)\n",
    "            print(\"Final Summary : \\n\",summary)\n",
    "            return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmT5a_2FgPwp"
   },
   "source": [
    "### **Map Step :**\n",
    "\n",
    "***\n",
    "\n",
    "* We input multiple smaller documents, after they have been divided into chunks, and operate over them with a MapReduceDocumentsChain. This chain sends the smaller text chunks to Large Language Model with map_prompt and creates summaries for all the text chunks.\n",
    "\n",
    "Map prompt in the below example:\n",
    "\n",
    "```\n",
    "map_prompt = \"\"\" Generate a concise summary of this text that includes the main points and key information.\n",
    "                {text}\n",
    "            \"\"\"\n",
    "```\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9FL9n9KgPwp"
   },
   "source": [
    "### **Reduce Step:**\n",
    "\n",
    "***\n",
    "\n",
    "* In reduce stage, after all the summaries of chunks are generated, we use 'combine_prompt' to combine these summaries into a single final summary.\n",
    "\n",
    "Combine prompt in the below example:\n",
    "\n",
    "```\n",
    "combine_prompt = \"\"\"Combine all these following texts and return a summary of combined text . Return a complete sentence:\n",
    "'''{text}'''\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7tm6YoQtgPwp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create LangChain's load_summarize_chain to do the map_reducing on the input document.\n",
    "\n",
    "content_handler = ContentHandlerTextSummarization()\n",
    "\n",
    "\n",
    "#Create a map_prompt and map_prompt_template for creating summaries of smaller chunks\n",
    "map_prompt = \"\"\" Generate a concise summary of this text that includes the main points and key information.\n",
    "                {text}\n",
    "            \"\"\"\n",
    "\n",
    "map_prompt_template = PromptTemplate(\n",
    "                        template=map_prompt,\n",
    "                        input_variables=[\"text\"]\n",
    "                      )\n",
    "\n",
    "\n",
    "#Create a combine_prompt and combine_prompt_template for creating a summary of all the summarized chunks\n",
    "combine_prompt = \"\"\"Combine all these following texts and return a summary of combined text. Return a complete sentence:\n",
    "'''{text}'''\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "combine_prompt_template = PromptTemplate(\n",
    "                            template=combine_prompt,\n",
    "                            input_variables=[\"text\"]\n",
    "                          )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHeuD4CEgPwp"
   },
   "source": [
    "### [Langchain Sagemaker Endpoint](https://python.langchain.com/docs/modules/model_io/models/llms/integrations/sagemaker.html)\n",
    "\n",
    "***\n",
    "It is a wrapper around custom Sagemaker Inference Endpoints.To use, you must supply the endpoint name from your deployed Sagemaker model & the region where it is deployed.\n",
    "\n",
    "* **endpoint_name: str** : The name of the endpoint from the deployed Sagemaker model.Must be unique within an AWS Region.\n",
    "* **region_name: str** : The aws region where the Sagemaker model is deployed, eg. `us-east-1`.\n",
    "* **credentials_profile_name: Optional[str]** :[Reference](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html) - The name of the profile in the ~/.aws/credentials or ~/.aws/config files, which has either access keys or role information specified. If not specified, the default credential profile or, if on an EC2 instance,credentials from IMDS will be used.\n",
    "* **content_handler: LLMContentHandler** : The content handler class that provides an input and output transform functions to handle formats between LLM and the endpoint.\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n005HPl9gPwp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Using Sagemaker Endpoint to define the Large Language model ( Falcon 7B Instruct BF16 )\n",
    "summary_model = SagemakerEndpoint(\n",
    "                    endpoint_name = falcon_endpoint_name,\n",
    "                    region_name = aws_region,\n",
    "                    model_kwargs = {\"parameters\":{\"max_new_tokens\": 100},},\n",
    "                    content_handler=content_handler,\n",
    "\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "of1q9R3QgPwp"
   },
   "source": [
    "***\n",
    "\n",
    "We use Langchain's [load_summarize_chain](https://python.langchain.com/docs/modules/chains/popular/summarize)  to generate our final summary.\n",
    "\n",
    "The below parameters are passed to load_summarize_chain:\n",
    "\n",
    "1. **llm** : The large language model to query the user input.\n",
    "2. **chain_type** : Type of chain, for summarization of input documents.\n",
    "3. **map_prompt** : Prompt for map step (Summarizing multiple smaller text chunks).\n",
    "4. **combine_prompt** : Prompt for Reduce Step (Combining all the chunks summaries into a single summary).\n",
    "5. **verbose** : Set to true to see all the intermediate steps before receiving the output.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0_CVJjHgPwp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating an object of load_summarize_chain which takes in chain_type, map_prompt, combine_prompt as parameters.\n",
    "summary_chain = load_summarize_chain(llm=summary_model,\n",
    "                                     chain_type=\"map_reduce\",\n",
    "                                     map_prompt=map_prompt_template,\n",
    "                                     combine_prompt=combine_prompt_template,\n",
    "                                     verbose = True\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akjrITf1gPwq"
   },
   "source": [
    "#### Input and Output Variables for Map Reduce Summary Chain\n",
    "***\n",
    "* **Input**\n",
    "    * `input_documents` : It accepts list of document objects.(splitted document chunks)\n",
    "* **Output**\n",
    "    * `output_text` : The final combined summary of all the summaries of text chunks.\n",
    "* [Langchain Map Reduce Summarize I/O Reference](https://python.langchain.com/docs/modules/chains/popular/summarize), [Github Reference](https://github.com/hwchase17/langchain/blob/6a64870ea05ad6a750b8753ce7477a5355539f0d/langchain/chains/combine_documents/map_reduce.py)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JUVw9x1tgPwq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_chain.invoke({\"input_documents\": input_documents}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPrJdoZLgPwq"
   },
   "source": [
    "## **6. Create an LLMChain for Text To Image conversion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-ESccy8gPwq"
   },
   "source": [
    "![TEXT_TO_IMAGE CHAIN](images/text_to_image.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6nQHucbgPwq"
   },
   "source": [
    "### **Content Handler For Stable Diffusion 2.1 Base**\n",
    "\n",
    "***\n",
    "* [A handler class](https://github.com/hwchase17/langchain/blob/6a64870ea05ad6a750b8753ce7477a5355539f0d/langchain/llms/sagemaker_endpoint.py#L64) to transform input from LLM to a format that SageMaker endpoint expects. Similarily,the class also handles transforming output from the SageMaker endpoint to a format that LLM class expects.\n",
    "\n",
    "   \n",
    "* **content_type: Optional[str]** :The MIME type of the input data passed to endpoint\n",
    "* **accepts: Optional[str]** : The MIME type of the response data returned from endpoint\"\n",
    "* **def transform_input(self, prompt: INPUT_TYPE, model_kwargs: Dict) -> bytes:** : Transforms the input to a format that model can accept as the request Body. Should return bytes or seekable file like object in the format specified in the content_type request header.\n",
    "* **def transform_output(self, output: bytes) -> OUTPUT_TYPE:** : Transforms the output from the model to string that the LLM class expects.\n",
    "* **LLMContentHandler(ContentHandlerBase[str, str]):** : Content handler for LLM class.\n",
    "\n",
    "#### **[NOTE : Input Output Reference for Sagemaker Stable Diffusion 2.1 Base](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart_text_to_image/Amazon_JumpStart_Text_To_Image.ipynb)**\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5Y0qaFkgPwq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Class that provides input and output transform functions to handle formats between LLM and the endpoint.\n",
    "\n",
    "class ContentHandlerStableDiffusion(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        if model_kwargs is None:\n",
    "            model_kwargs = {}\n",
    "\n",
    "        input_payload = {\n",
    "            \"text_prompts\": [{\"text\":prompt}],\n",
    "            **model_kwargs\n",
    "        }\n",
    "\n",
    "        input_str = json.dumps(input_payload)\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "\n",
    "        #Displaying the output image stored in the json key 'generated_image'\n",
    "        decode_and_show(response_json['generated_image'])\n",
    "\n",
    "        #Returning the prompt from json output\n",
    "        return response_json[\"prompt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3Ou0bIFgPwr"
   },
   "source": [
    "#### Input and Output Variables for Stable Diffusion\n",
    "***\n",
    "* **Input**\n",
    "    * **`prompt_specification`**  : User defines custom image styles for creating visually appealing images.\n",
    "    * **`output_text`** : The final summary generated by the Langchain MapReduceDocumentChain.\n",
    "* **Output**\n",
    "    * json object having image in **'`generated_image`.'**\n",
    "* [Langchain Map Reduce Summarize I/O Reference](https://python.langchain.com/docs/modules/chains/popular/summarize), [Github Reference](https://github.com/hwchase17/langchain/blob/6a64870ea05ad6a750b8753ce7477a5355539f0d/langchain/chains/combine_documents/map_reduce.py)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFCqEr6dgPwr"
   },
   "source": [
    "### LLMChain\n",
    "***\n",
    "* It consists of a image generation prompt template, and stable diffusion text to image LLM. This chain takes  \"output_text\", and \"prompt_specification\" as input variables, and uses the PromptTemplate to format them into a prompt. It then passes that to the model.\n",
    "\n",
    "* [Understanding Langchain LLMChain](https://python.langchain.com/docs/modules/chains/foundational/llm_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-7aytwigPwr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create LangChain's LLMChain to do convert the output summary into an image.\n",
    "\n",
    "\n",
    "#Create an object of ContentHandlerStableDiffusion\n",
    "content_handler = ContentHandlerStableDiffusion()\n",
    "\n",
    "#Define a custom prompt and template for the image generation.\n",
    "image_generation_prompt_template = \"Convert this text {output_text} into image with {prompt_specification}\"\n",
    "\n",
    "#output_text is the output parameter of Langchain summary MapReduceDocumentsChain.\n",
    "#prompt_specification is the image styles defined by the User for creating visually appealing images.\n",
    "image_generation_prompt= PromptTemplate(\n",
    "                            input_variables=[\"output_text\", \"prompt_specification\"],\n",
    "                            template=image_generation_prompt_template\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the 'InferenceComponentName' value with the name of the model you deployed for the Stable Diffusion XL 1.0 (open-source) model. You can retrieve that by navigating on the left side through Home -> Deployments -> Endpoints. Select the model name that starts with \"model-imagegeneration-stabilityai-stable-diffus-\" and replace its value with the place holder `<model name>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpjDIDRXgPwr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Using Sagemaker Endpoint to define the Stable Diffusion 2.1 base model\n",
    "text_to_image_model = SagemakerEndpoint(\n",
    "                        endpoint_name=sd_endpoint_name,\n",
    "                        endpoint_kwargs={\"InferenceComponentName\":'<model name>'},\t\n",
    "                        region_name=aws_region,\n",
    "                        content_handler=content_handler\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35Vd0x56gPws",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#LLMChain to chain the image_generation_prompt and texttoimage_model\n",
    "#Returning \"prompt\" from the Stable Diffusion output json object.\n",
    "text_to_image_chain = LLMChain(llm=text_to_image_model,\n",
    "                               prompt=image_generation_prompt,\n",
    "                               output_key=\"prompt\"\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tIyH6sWogPws",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Helper function to display the generated image\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "def decode_and_show(model_response) -> None:\n",
    "    \"\"\"\n",
    "    Decodes and displays an image from SDXL output\n",
    "\n",
    "    Args:\n",
    "        model_response (GenerationResponse): The response object from the deployed SDXL model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    image = Image.open(io.BytesIO(base64.b64decode(model_response)))\n",
    "    display(image)\n",
    "    image.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZK0wfmrgPws"
   },
   "source": [
    "### **7.Combine the chains using Langchain SequentialChain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85iijBTAgPws"
   },
   "source": [
    "![COMBINATION_OF_CHAINS](images/overall_chain.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoMmOvX8gPws"
   },
   "source": [
    "### Sequential chain to combine the above two chains.\n",
    "\n",
    "Sequential chains allows to connect summary_chain and text_to_image_chain , and executes creating a pipeline for summarization and text to image generation. We start the overall chain, and at the end we get the final image from large input document chunks.\n",
    "\n",
    "The flow is:\n",
    "\n",
    "Large input document -> Combined Summary from Mapreduce load_summarize_chain -> conversion of final summary into an image with specific user image style requirements.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXmXW--igPws",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Combine the summary_chain and text_to_image_chain using SequentialChain\n",
    "\n",
    "prompt_specification = \"Highly Detailed, hyperrealistic, dynamic lighting, octane render, fancy highlights, intricate detail\"\n",
    "\n",
    "#This is the overall chain where we run these summary_chain and text_to_image_chain in sequence\n",
    "overall_chain = SequentialChain(chains = [summary_chain, text_to_image_chain],\n",
    "                                input_variables=[\"input_documents\",\"prompt_specification\"] ,\n",
    "                                output_variables=[\"prompt\"] #From text_to_image chain.\n",
    "                                )\n",
    "\n",
    "\n",
    "#Run the combined chain by passing inputs to the chains\n",
    "print(overall_chain({'prompt_specification':prompt_specification, 'input_documents':input_documents},return_only_outputs=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "colab": {
   "provenance": []
  },
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
