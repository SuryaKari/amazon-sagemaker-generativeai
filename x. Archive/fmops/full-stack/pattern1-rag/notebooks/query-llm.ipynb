{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG) Query to LLM\n",
    "\n",
    "In this notebook, we will show you how to query the LLM with RAG techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in parameters\n",
    "Replace placeholder parameters below with SageMaker Jumpstart model endpoints with OpenSearch domain endpoint and Prompt Firehose name retrieved from CDK deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace parameters with your own values, these are example values\n",
    "\n",
    "text_model_endpoint = \"jumpstart-dft-hf-llm-falcon-40b-bf16\"\n",
    "embed_model_endpoint = \"jumpstart-dft-hf-textembedding-gpt-j-6b-fp16\"\n",
    "opensearch_domain_endpoint = \"vpc-opensearchdomai-dtvvqhrhsqtc-avpib3sgtuvbynuwyqgwutrya4.us-east-1.es.amazonaws.com\"\n",
    "fh_prompt_name = \"BackendStack-FirehosePrompts-m7cR7FRnyw3O\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install boto3==1.28.17\n",
    "!pip3 install streamlit==1.27.2\n",
    "!pip3 install langchain==0.0.317\n",
    "!pip3 install opensearch-py==2.4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all required libraries such as boto3, langchain etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from boto3.dynamodb.conditions import Key\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from langchain import SagemakerEndpoint, PromptTemplate\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from opensearchpy import RequestsHttpConnection, AWSV4SignerAuth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure boto3 clients and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwclient = boto3.client('cloudwatch')\n",
    "fhclient = boto3.client('firehose')\n",
    "credentials = boto3.Session().get_credentials()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define DynamoDB and CloudWatch functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def put_ddb_item(item):\n",
    "#     try:\n",
    "#         table.put_item(Item=item)\n",
    "#     except ClientError as err:\n",
    "#         logger.error(err.response['Error']['Code'], err.response['Error']['Message'])\n",
    "#         raise\n",
    "\n",
    "# def get_ddb_item(id):\n",
    "#     try:\n",
    "#         items = table.query(KeyConditionExpression=Key('id').eq(id))['Items'][0]\n",
    "#         return items\n",
    "#     except ClientError as err:\n",
    "#         logger.error(err.response['Error']['Code'], err.response['Error']['Message'])\n",
    "#         raise\n",
    "\n",
    "def put_cw_metric(cwclient, score):\n",
    "    try:\n",
    "        cwclient.put_metric_data(\n",
    "            Namespace='rag',\n",
    "            MetricData=[\n",
    "                {\n",
    "                    'MetricName': 'similarity',\n",
    "                    'Value': score,\n",
    "                    'Unit': 'None',\n",
    "                    'StorageResolution': 1\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "    except ClientError as err:\n",
    "        logger.error(err.response['Error']['Code'], err.response['Error']['Message'])\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Langchain input and output handlers for Sagemaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextContentHandler(LLMContentHandler):\n",
    "    \"\"\"\n",
    "    encode input string as utf-8 bytes, read the generated text\n",
    "    from the output\n",
    "    \"\"\"\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs = {}) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[0]['generated_text']\n",
    "\n",
    "class EmbeddingsContentHandler(EmbeddingsContentHandler):\n",
    "    \"\"\"\n",
    "    encode input string as utf-8 bytes, read the embeddings\n",
    "    from the output\n",
    "    \"\"\"\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, inputs: list[str], model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\"text_inputs\": inputs, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes):\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"embedding\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions for embedding and text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sagemaker_embeddings(endpoint_name):\n",
    "    # create a content handler object which knows how to serialize\n",
    "    # and deserialize communication with the model endpoint\n",
    "    content_handler = EmbeddingsContentHandler()\n",
    "\n",
    "    # read to create the Sagemaker embeddings, we are providing\n",
    "    # the Sagemaker endpoint that will be used for generating the\n",
    "    # embeddings to the class\n",
    "    embeddings = SagemakerEndpointEmbeddings(\n",
    "        endpoint_name=endpoint_name,\n",
    "        region_name=region, \n",
    "        content_handler=content_handler\n",
    "    )\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# Functiion to do vector search and get context from opensearch. Returns list of documents\n",
    "def get_context_from_opensearch(query, endpoint_name, opensearch_domain_endpoint, opensearch_index):\n",
    "\n",
    "    credentials = boto3.Session().get_credentials()\n",
    "    region = boto3.Session().region_name\n",
    "    auth = AWSV4SignerAuth(credentials, region, \"es\")\n",
    "    opensearch_endpoint = f\"https://{opensearch_domain_endpoint}\"\n",
    "    docsearch = OpenSearchVectorSearch(\n",
    "        index_name=opensearch_index,\n",
    "        embedding_function=create_sagemaker_embeddings(endpoint_name),\n",
    "        opensearch_url=opensearch_endpoint,\n",
    "        http_auth = auth,\n",
    "        use_ssl = True,\n",
    "        verify_certs = True,\n",
    "        connection_class = RequestsHttpConnection,\n",
    "        is_aoss=False\n",
    "    )\n",
    "\n",
    "    # docsearch = OpenSearchVectorSearch(\n",
    "    #     index_name=opensearch_index,\n",
    "    #     embedding_function=create_sagemaker_embeddings(endpoint_name),\n",
    "    #     opensearch_url=opensearch_endpoint,\n",
    "    #     is_aoss=False\n",
    "    # )\n",
    "    docs_with_scores = docsearch.similarity_search_with_score(query, k=3, vector_field=\"embedding\", text_field=\"passage\")\n",
    "    for d in docs_with_scores:\n",
    "        score = d[1]\n",
    "        put_cw_metric(cwclient, score)\n",
    "    docs = [doc[0] for doc in docs_with_scores]\n",
    "    logger.info(f\"docs received from opensearch:\\n{docs}\")\n",
    "    return docs # return list of matching docs\n",
    "\n",
    "# Function to combine the context from vector search, combine with question and query sage maker deployed model\n",
    "def call_sm_text_generation_model(query, context, endpoint_name):\n",
    "\n",
    "    # create a content handler object which knows how to serialize\n",
    "    # and deserialize communication with the model endpoint\n",
    "    content_handler = TextContentHandler()\n",
    "    \n",
    "    ## Query to sagemaker endpoint to generate a response from query and context\n",
    "    llm = SagemakerEndpoint(\n",
    "        endpoint_name=endpoint_name,\n",
    "        region_name=region,\n",
    "        content_handler=content_handler,\n",
    "        endpoint_kwargs={'CustomAttributes': 'accept_eula=true'}\n",
    "    )\n",
    "    prompt_template = \"\"\"Answer based on context:\\n\\n{context}\\n\\n{question}\"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    logger.info(f\"prompt sent to llm = \\\"{prompt}\\\"\")\n",
    "    chain = load_qa_chain(llm=llm, prompt=prompt)\n",
    "    answer = chain({\"input_documents\": context, \"question\": query}, return_only_outputs=True)['output_text']\n",
    "    logger.info(f\"answer received from llm,\\nquestion: \\\"{query}\\\"\\nanswer: \\\"{answer}\\\"\")\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter your query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER YOUR QUERY AS A STRING BEFORE RUNNING THIS CELL\n",
    "query = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = get_context_from_opensearch(query, embed_model_endpoint, opensearch_domain_endpoint, \"embeddings\")\n",
    "context_formatted =  [{\"page_content\": doc.page_content} for doc in context]\n",
    "print(f\"Found {str(len(context))} similar documents\")\n",
    "\n",
    "answer = call_sm_text_generation_model(query, context, text_model_endpoint)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send prompt data to Firehose for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh_stream_records = []\n",
    "embedding = create_sagemaker_embeddings(embed_model_endpoint).embed_query(query)\n",
    "fh_stream_records.append({'Data': (str(embedding)+ \"\\n\").encode('utf-8')})\n",
    "fhclient.put_record_batch( DeliveryStreamName=fh_prompt_name, Records=fh_stream_records)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
