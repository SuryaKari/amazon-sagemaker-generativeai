{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 3.0\n%worker_type G.2X\n%number_of_workers 10\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.38.1 \nCurrent idle_timeout is 2800 minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 3.0\nPrevious worker type: G.1X\nSetting new worker type to: G.2X\nPrevious number of workers: 5\nSetting new number of workers to: 10\nAuthenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::102165494304:role/glueinteractive\nTrying to create a Glue session for the kernel.\nWorker Type: G.2X\nNumber of Workers: 10\nSession ID: 2207338f-4b27-4bcd-b999-e8be81530897\nJob Type: glueetl\nApplying the following default arguments:\n--glue_kernel_version 0.38.1\n--enable-glue-datacatalog true\nWaiting for session 2207338f-4b27-4bcd-b999-e8be81530897 to get into ready status...\nSession 2207338f-4b27-4bcd-b999-e8be81530897 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Create a DynamicFrame from a table in the AWS Glue Data Catalog and display its schema\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "df = spark.read.option(\"recursiveFileLookup\", \"true\").text('s3://cdkstack-documentsbucket9ec9deb9-sbbf9n4wdhze/embeddingarchive/')",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dfP = spark.read.option(\"recursiveFileLookup\", \"true\").text('s3://cdkstack-documentsbucket9ec9deb9-sbbf9n4wdhze/promptarchive/')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df.count()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "835\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dfP.count()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "4\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import split, col, regexp_replace, transform\ndf = df.withColumn(\"value\", regexp_replace(\"value\", r'(\\[)', '')).withColumn(\"value\", regexp_replace(\"value\", r'(])', ''))\ndfP = dfP.withColumn(\"value\", regexp_replace(\"value\", r'(\\[)', '')).withColumn(\"value\", regexp_replace(\"value\", r'(])', ''))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import split, col\ndf = df.select(split(col(\"value\"),\",\").alias(\"EmbedArray\")).drop(\"value\")\ndfP = dfP.select(split(col(\"value\"),\",\").alias(\"EmbedArray\")).drop(\"value\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df = df.withColumn(\"EmbedArray\", transform(col(\"EmbedArray\"), lambda x: x.cast(\"float\")))\ndf = df.withColumn(\"EmbedArray\", col(\"EmbedArray\").cast(\"array<float>\"))\ndfP = dfP.withColumn(\"EmbedArray\", transform(col(\"EmbedArray\"), lambda x: x.cast(\"float\")))\ndfP = dfP.withColumn(\"EmbedArray\", col(\"EmbedArray\").cast(\"array<float>\"))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.ml.linalg import Vectors",
			"metadata": {
				"trusted": true
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.ml.functions import array_to_vector",
			"metadata": {
				"trusted": true
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df = df.select(array_to_vector('EmbedArray').alias('EmbedArray'))\ndfP = dfP.select(array_to_vector('EmbedArray').alias('EmbedArray'))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.ml.feature import VectorAssembler\nassembler = VectorAssembler(\n  inputCols=[\"EmbedArray\"], outputCol=\"features\"\n)\n\ndfTrain = assembler.transform(df).drop('EmbedArray')\ndfTrainP = assembler.transform(dfP).drop('EmbedArray')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.ml.feature import PCA\npca = PCA(k=100, inputCol=\"features\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "pca.setOutputCol(\"pca_features\")\npca_model = pca.fit(dfTrain)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "pca_model.setOutputCol(\"output\")\ndfPca = pca_model.transform(dfTrain)\ndfPcaP = pca_model.transform(dfTrainP)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import numpy as np",
			"metadata": {
				"trusted": true
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.ml.clustering import KMeans",
			"metadata": {
				"trusted": true
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "kmeans = KMeans(k=10)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "kmeans_model = kmeans.fit(dfPca)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "kmeans_model.setPredictionCol(\"newPrediction\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 20,
			"outputs": [
				{
					"name": "stdout",
					"text": "KMeansModel: uid=KMeans_5ed8cbc668c1, k=10, distanceMeasure=euclidean, numFeatures=4096\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dfKmeans = kmeans_model.transform(dfPca).select(\"features\", \"newPrediction\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 22,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dfKmeansP = kmeans_model.transform(dfPcaP).select(\"features\", \"newPrediction\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 21,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.ml.evaluation import ClusteringEvaluator\nevaluator = ClusteringEvaluator(predictionCol='newPrediction', featuresCol='features', \\\n                                metricName='silhouette', distanceMeasure='squaredEuclidean')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 23,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "score=evaluator.evaluate(dfKmeans)\nscore",
			"metadata": {
				"trusted": true
			},
			"execution_count": 24,
			"outputs": [
				{
					"name": "stdout",
					"text": "0.1058243792342071\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "scoreP=evaluator.evaluate(dfKmeansP)\nscoreP",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 25,
			"outputs": [
				{
					"name": "stdout",
					"text": "-0.13371579342985535\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dfKmeansP.printSchema()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 26,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- features: vector (nullable = true)\n |-- newPrediction: integer (nullable = false)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dfKmeansP.head(1)[0]['newPrediction']",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 28,
			"outputs": [
				{
					"name": "stdout",
					"text": "5\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import pyspark.sql.functions as F\nfrom pyspark.sql.types import IntegerType\nl_clusters = kmeans_model.clusterCenters()\n# Let's convert the list of centers to a dict, each center is a list of float\nd_clusters = {int(i):[float(l_clusters[i][j]) for j in range(len(l_clusters[i]))] for i in range(len(l_clusters))}\n\n# Let's create a dataframe containing the centers and their coordinates\ndf_centers = spark.sparkContext.parallelize([(k,)+(v,) for k,v in d_clusters.items()]).toDF(['prediction','center'])\n\ndfKmeansP = dfKmeansP.withColumn('prediction',F.col('newPrediction').cast(IntegerType()))\ndfKmeansP = dfKmeansP.join(df_centers,on='prediction',how='left')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 33,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.types import FloatType\nget_dist = F.udf(lambda features, center : float(features.squared_distance(center)),FloatType())\ndfKmeansP = dfKmeansP.withColumn('dist',get_dist(F.col('features'),F.col('center')))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 34,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dfKmeansP.printSchema()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 36,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- prediction: integer (nullable = false)\n |-- features: vector (nullable = true)\n |-- newPrediction: integer (nullable = false)\n |-- center: array (nullable = true)\n |    |-- element: double (containsNull = true)\n |-- dist: float (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dfKmeansP.head(1)[0]['dist']",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 37,
			"outputs": [
				{
					"name": "stdout",
					"text": "551.3375854492188\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import mean as _mean, stddev as _stddev\n\ndf_stats = dfKmeansP.select(\n    _mean(col('dist')).alias('mean'),\n    _stddev(col('dist')).alias('std')\n).collect()\n\nmean = df_stats[0]['mean']\nstd = df_stats[0]['std']",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 38,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "mean",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 39,
			"outputs": [
				{
					"name": "stdout",
					"text": "669.6846122741699\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "std",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 40,
			"outputs": [
				{
					"name": "stdout",
					"text": "389.3161660099286\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}