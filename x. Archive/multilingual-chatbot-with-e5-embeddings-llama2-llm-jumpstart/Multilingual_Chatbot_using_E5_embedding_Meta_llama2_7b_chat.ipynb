{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilingual chatbot using E5 multilingual embeddings model and Meta llama2 7-b chat LLM in Sagemaker Jumpstart\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### This notebook has been tested in us-east-1. Use Data Science 3.0 Image\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will demonstrate how to use [**Llama-2-7b Chat**](https://ai.meta.com/llama/) to answer questions using a library of documents in 3 different languages as a reference, by using document embeddings and retrieval. Unlike other RAG solutions, embeddings will be generated and combined with the embedding model, Multilinigual E5 Large [Multilingual E5 Large](https://huggingface.co/intfloat/multilingual-e5-large) to identify the nearest neighbors, all from a single endpoint in this solution. The source documents are in 3 languages: English, Spanish and Italian. The documents are Amazon SageMaker FAQs sourced from [Amazon SageMaker FAQ](https://aws.amazon.com/sagemaker/faqs/?nc1=h_ls)\n",
    "\n",
    "\n",
    "To perform inference on the [Llama models](https://ai.meta.com/llama/), you need to pass custom_attributes='accept_eula=true' as part of header. This means you have read and accept the end-user-license-agreement (EULA) of the model. EULA can be found in model card description or from this [webpage](https://ai.meta.com/resources/models-and-libraries/llama-downloads/).\n",
    "\n",
    "Note: Custom_attributes used to pass EULA are key/value pairs. The key and value are separated by '=' and pairs are separated by ';'. If the user passes the same key more than once, the last value is kept and passed to the script handler (i.e., in this case, used for conditional logic). For example, if 'accept_eula=false; accept_eula=true' is passed to the server, then 'accept_eula=true' is kept and passed to the script handler.\n",
    "\n",
    "Other Retrieval Augmented Generation Solutions - \n",
    "- [Question Answering using LangChain and Cohere's Generate and Embedding Models from SageMaker JumpStart](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/question_answering_retrieval_augmented_generation/question_answering_Cohere%2Blangchain_jumpstart.ipynb)\n",
    "- [Question Answering based on Custom Dataset](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/question_answering_retrieval_augmented_generation/question_answering_jumpstart_knn.ipynb)\n",
    "- [Question Answering based on Custom Dataset with Open-sourced LangChain Library](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/question_answering_retrieval_augmented_generation/question_answering_langchain_jumpstart.ipynb)\n",
    "- [Question Answering using LLama-2, Pinecone & Custom Dataset](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/question_answering_retrieval_augmented_generation/question_answering_pinecone_llama-2_jumpstart.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Deploy Llama-2 7 Billion Chat Model in SageMaker JumpStart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Run this cell if you are running the code in your local IDE\n",
    "!pip install -qU \\\n",
    "    sagemaker \\\n",
    "    pinecone-client==2.2.1 \\\n",
    "    ipywidgets==7.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we will initialize all of the SageMaker session variables we'll need to use throughout the walkthrough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "my_model = JumpStartModel(model_id=\"meta-textgeneration-llama-2-7b-f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a `ml.g5.4xlarge` instance to deploy our Llama-2-7 billion model. We can find pricing for all instances [here](https://aws.amazon.com/sagemaker/pricing/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------!"
     ]
    }
   ],
   "source": [
    "predictor_llm = my_model.deploy(initial_instance_count=1, instance_type=\"ml.g5.4xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To gain an understanding of the necessity for a retrieval-augmented generation (RAG) approach in addressing the question and answering problem, please refer to this  [question_answering_pinecone_llama-2_jumpstart.ipynb](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/question_answering_retrieval_augmented_generation/question_answering_pinecone_llama-2_jumpstart.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Use Text Embeddings to identify the correct context from the documents including documents of the language of choice based on the question, and use them along with prompt and question to query LLM\n",
    "\n",
    "\n",
    "We plan to use document embeddings to fetch the most relevant documents in our document knowledge library and combine them with the prompt that we provide to LLM. The main difference here is the documents and their corresponding embeddings are in English, Spanish and Italian. \n",
    "\n",
    "\n",
    "To achieve that, we will do following.\n",
    "\n",
    "* Running a text embedding model training job. The training job will generate embeddings for dataset provided and save them along with the model. These embeddings will be utilized during inference to find the nearest neighbors for an input sentence. The nearest neighbor is based on the cosine similarity between the input sentence embedding and already computed sentence embeddings during the training job. To get more information please refer to [text-embedding-sentence-similarity.ipynb](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/question_answering_retrieval_augmented_generation/text-embedding-sentence-similarity.ipynb)\n",
    "* Query the text embedding model endpoint created above to Identify top K most relevant documents based on user query\n",
    "* Combine the retrieved documents with prompt and question and send them into LLM.\n",
    "\n",
    "Note: We are saving the dataset here with the model only to get the most similar document unlike the other RAG solutions. \n",
    "\n",
    "\n",
    "Note: The retrieved document/text should be large enough to contain enough information to answer a question; but small enough to fit into the LLM prompt -- maximum sequence length of 1024 tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To train and host on Amazon Sagemaker, we need to setup and authenticate the use of AWS services. Here, we use the execution role associated with the current notebook instance as the AWS account role with SageMaker access. It has necessary permissions, including access to your data in S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are using the **huggingface-sentencesimilarity-multilingual-e5-large** model to get embeddings. Multilingual-E5-large model is initialized from xlm-roberta-large and continually trained on a mixture of multilingual datasets. It supports 100 languages from xlm-roberta, but low-resource languages may see performance degradation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"huggingface-sentencesimilarity-multilingual-e5-large\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./data.csv to s3://sagemaker-us-east-1-385888608451/jumpstart-example-multilingual-training/data/data.csv\n"
     ]
    }
   ],
   "source": [
    "# In this section, we'll be fetching and prepping the Amazon_SageMaker_FAQs dataset to utilize it in finding the nearest neighbour to an input question. \n",
    "# We will input FAQs in English, Spanish and Italian and create a single data file which will be used for creating embeddings\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#!aws s3 cp s3://jumpstart-cache-prod-us-west-2/training-datasets/Amazon_SageMaker_FAQs/Amazon_SageMaker_FAQs.csv Amazon_SageMaker_FAQs.csv\n",
    "\n",
    "#Reading FAQs in English, Spanish and Italian\n",
    "#English FAQs\n",
    "df_faq_en = pd.read_csv(\"Amazon_SageMaker_FAQs.csv\", header=None)\n",
    "#Spanish FAQs\n",
    "df_faq_es = pd.read_csv(\"Amazon_SageMaker_ES_FAQ.csv\", header=None)\n",
    "#Italian FAQs\n",
    "df_faq_it = pd.read_csv(\"Amazon_SageMaker_IT_FAQ.csv\", header=None)\n",
    "#Create Single FAQ file of all languages\n",
    "pd.concat([df_faq_en, df_faq_es, df_faq_it]).to_csv('Amazon_SageMaker_FAQ_Multilingual.csv', index=False)\n",
    "\n",
    "# Preparing the Data in the required format\n",
    "\n",
    "data = pd.read_csv(\"Amazon_SageMaker_FAQ_Multilingual.csv\", names=[\"Questions\", \"Answers\"])\n",
    "data[\"id\"] = data.index\n",
    "\n",
    "data_req = data[[\"id\", \"Answers\"]]\n",
    "\n",
    "data_req.to_csv(\"data.csv\", index=False, header=False)\n",
    "\n",
    "# Uploading the data in required format to s3 Bucket\n",
    "output_bucket = sess.default_bucket()\n",
    "#You can modify this to your own prefix\n",
    "output_prefix = \"jumpstart-example-multilingual-training\"\n",
    "\n",
    "s3_output_location = f\"s3://{output_bucket}/{output_prefix}/output\"\n",
    "training_dataset_s3_path = f\"s3://{output_bucket}/{output_prefix}/data/data.csv\"\n",
    "\n",
    "!aws s3 cp data.csv {training_dataset_s3_path}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 Getting the Embeddings for the Input data using Training Job for Multilingual FAQs and store them in the Multilingual E5 Large embeddings model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: multilingual-e5-large-2023-10-28-02-06-24-699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_seq_length': 'None', 'batch_size': '128', 'store_text_with_embedding': 'True'}\n",
      "2023-10-28 02:06:25 Starting - Starting the training job...\n",
      "2023-10-28 02:06:42 Starting - Preparing the instances for training......\n",
      "2023-10-28 02:07:54 Downloading - Downloading input data.........\n",
      "2023-10-28 02:09:24 Training - Downloading the training image........................\n",
      "2023-10-28 02:13:15 Training - Training image download completed. Training in progress....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-10-28 02:13:45,237 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-10-28 02:13:45,252 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-28 02:13:45,262 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-10-28 02:13:45,264 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-10-28 02:13:45,495 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/datasets/datasets-2.13.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nltk/nltk-3.8.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sentence-transformers/sentence-transformers-2.2.2.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.8-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from datasets==2.13.1->-r requirements.txt (line 1)) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets==2.13.1->-r requirements.txt (line 1)) (2023.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets==2.13.1->-r requirements.txt (line 1)) (1.5.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets==2.13.1->-r requirements.txt (line 1)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from datasets==2.13.1->-r requirements.txt (line 1)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from datasets==2.13.1->-r requirements.txt (line 1)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets==2.13.1->-r requirements.txt (line 1)) (11.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets==2.13.1->-r requirements.txt (line 1)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets==2.13.1->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from datasets==2.13.1->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from datasets==2.13.1->-r requirements.txt (line 1)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from datasets==2.13.1->-r requirements.txt (line 1)) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets==2.13.1->-r requirements.txt (line 1)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk==3.8.1->-r requirements.txt (line 2)) (8.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk==3.8.1->-r requirements.txt (line 2)) (2022.10.31)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk==3.8.1->-r requirements.txt (line 2)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.9/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 3)) (4.26.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 3)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision in /opt/conda/lib/python3.9/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 3)) (0.14.1+cu117)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.9/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 3)) (1.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 3)) (1.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.9/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 3)) (0.1.97)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.13.1->-r requirements.txt (line 1)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.13.1->-r requirements.txt (line 1)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.13.1->-r requirements.txt (line 1)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.13.1->-r requirements.txt (line 1)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.13.1->-r requirements.txt (line 1)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.13.1->-r requirements.txt (line 1)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.13.1->-r requirements.txt (line 1)) (1.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.13.1->-r requirements.txt (line 1)) (3.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.13.1->-r requirements.txt (line 1)) (4.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.13.1->-r requirements.txt (line 1)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.13.1->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.13.1->-r requirements.txt (line 1)) (2022.12.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2->-r requirements.txt (line 3)) (0.13.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==2.13.1->-r requirements.txt (line 1)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==2.13.1->-r requirements.txt (line 1)) (2022.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->sentence-transformers==2.2.2->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.9/site-packages (from torchvision->sentence-transformers==2.2.2->-r requirements.txt (line 3)) (9.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets==2.13.1->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sentence-transformers\u001b[0m\n",
      "\u001b[34mBuilding wheel for sentence-transformers (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for sentence-transformers (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125925 sha256=61697822cdbfa2d56c64706d90201da213692b3d751ce410dfa2dda5dd01ffda\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/86/c3/54/0f2dac9a42eb7def498774c6dfcd63b191f02632e1ff384764\u001b[0m\n",
      "\u001b[34mSuccessfully built sentence-transformers\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sagemaker-jumpstart-script-utilities, nltk, sentence-transformers, datasets\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.9.0\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.9.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.9.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed datasets-2.13.1 nltk-3.8.1 sagemaker-jumpstart-script-utilities-1.1.8 sentence-transformers-2.2.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-10-28 02:13:50,152 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-10-28 02:13:50,152 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-10-28 02:13:50,168 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-28 02:13:50,193 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-28 02:13:50,220 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-28 02:13:50,242 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": \"128\",\n",
      "        \"max_seq_length\": \"None\",\n",
      "        \"store_text_with_embedding\": \"True\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"model\": {\n",
      "            \"ContentType\": \"application/x-sagemaker-model\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"multilingual-e5-large-2023-10-28-02-06-24-699\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":\"128\",\"max_seq_length\":\"None\",\"store_text_with_embedding\":\"True\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"code\",\"model\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"model\":\"/opt/ml/input/data/model\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":\"128\",\"max_seq_length\":\"None\",\"store_text_with_embedding\":\"True\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"multilingual-e5-large-2023-10-28-02-06-24-699\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"128\",\"--max_seq_length\",\"None\",\"--store_text_with_embedding\",\"True\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=128\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SEQ_LENGTH=None\u001b[0m\n",
      "\u001b[34mSM_HP_STORE_TEXT_WITH_EMBEDDING=True\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 transfer_learning.py --batch_size 128 --max_seq_length None --store_text_with_embedding True\u001b[0m\n",
      "\u001b[34m[2023-10-28 02:13:51.735: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-10-28 02:13:51,739 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-10-28 02:13:51,762 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-ff0b3ef7a0ee310c/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d...\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 1/1 [00:00<00:00, 13888.42it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 1/1 [00:00<00:00, 1838.80it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-ff0b3ef7a0ee310c/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:00<00:00, 1045.18it/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/465 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 465/465 [00:05<00:00, 89.09 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (0/1 shards):   0%|          | 0/465 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (1/1 shards): 100%|██████████| 465/465 [00:00<00:00, 136598.36 examples/s]\u001b[0m\n",
      "\u001b[34m2023-10-28 02:14:30,542 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-10-28 02:14:30,542 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-10-28 02:14:30,542 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-10-28 02:14:36 Uploading - Uploading generated training model\n",
      "2023-10-28 02:15:57 Completed - Training job completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.g5.2xlarge.\n",
      "INFO:sagemaker:Creating model with name: multilingual-e5-large-2023-10-28-02-16-11-553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training seconds: 484\n",
      "Billable seconds: 484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating endpoint-config with name multilingual-e5-large-2023-10-28-02-16-11-545\n",
      "INFO:sagemaker:Creating endpoint with name multilingual-e5-large-2023-10-28-02-16-11-545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!"
     ]
    }
   ],
   "source": [
    "from sagemaker import hyperparameters\n",
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "\n",
    "# Retrieve the default hyper-parameters for the model\n",
    "hyperparameters = hyperparameters.retrieve_default(model_id=model_id, model_version=\"*\")\n",
    "\n",
    "# [Optional] Override default hyperparameters with custom values\n",
    "# Store embeddings in model is \"True\" by default\n",
    "# default instance \n",
    "# max_seq_length parameter is the max sequence length of the input to process by the embedding model. The default None value results in using the default max_seq_length for the model.\n",
    "hyperparameters[\"batch_size\"] = \"128\"\n",
    "print(hyperparameters)\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id, hyperparameters=hyperparameters, output_path=s3_output_location\n",
    ")\n",
    "\n",
    "# Launch a SageMaker Training job by passing s3 path of the data\n",
    "\n",
    "estimator.fit({\"training\": f\"s3://{output_bucket}/{output_prefix}/data\"}, logs=True)\n",
    "\n",
    "# Use the estimator from the previous step to deploy to a SageMaker endpoint\n",
    "predictor_nn = estimator.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.3. Deploy & run Inference on the model to get nearest neighbor\n",
    "\n",
    "You can make queries to the endpoint using a JSON payload containing a batch of input texts, to find the nearest neighbors of the input text from the dataset which is provided during the training job.\n",
    "\n",
    "* **queries:** Provide the list of inputs for which to find the closest match from the training data\n",
    "* **top_k:** The number of closest match to find from the training data\n",
    "* **mode:** Supply it as \"nn_train_data\" for getting the nearest neighbors to input queries within the dataset provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input Question is: Qué es Amazon SageMaker Autopilot?\n",
      "The Corresponding Answer is: El piloto automático de Amazon SageMaker es la primera función de aprendizaje automático automatizado del sector que le brinda un control y una visibilidad totales de sus modelos de aprendizaje automático. SageMaker Autopilot inspecciona automáticamente los datos sin procesar, aplica procesadores de funciones, selecciona el mejor conjunto de algoritmos, entrena y ajusta varios modelos, realiza un seguimiento de su rendimiento y, a continuación, clasifica los modelos en función del rendimiento, todo ello con solo unos pocos clics. El resultado es el modelo con mejor rendimiento, que se puede implementar en una fracción del tiempo que normalmente se necesita para entrenar el modelo. Obtiene una visibilidad total de cómo se creó el modelo y qué contiene, y SageMaker Autopilot se integra con Amazon SageMaker Studio. Puede explorar hasta 50 modelos diferentes generados por SageMaker Autopilot dentro de SageMaker Studio, por lo que es fácil elegir el modelo que mejor se adapte a su caso de uso. Personas sin experiencia en aprendizaje automático pueden utilizar SageMaker Autopilot para crear un modelo con facilidad, o desarrolladores experimentados pueden utilizarlo para desarrollar rápidamente un modelo base que los equipos puedan seguir iterando.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "newline = \"\\n\"\n",
    "predictor_nn.serializer = JSONSerializer()\n",
    "predictor_nn.content_type = \"application/json\"\n",
    "\n",
    "payload_nearest_neighbour = {\n",
    "    \"queries\": [\"Qué es Amazon SageMaker Autopilot?\"],\n",
    "    \"top_k\": 1,\n",
    "    \"mode\": \"nn_train_data\",\n",
    "    \"return_text\": True,\n",
    "}\n",
    "\n",
    "response = predictor_nn.predict(payload_nearest_neighbour)\n",
    "\n",
    "question = payload_nearest_neighbour[\"queries\"][0]\n",
    "answer = response[0][0][\"text\"]\n",
    "# Relating the Input Question with the Answer\n",
    "print(f\"The input Question is: {question}{newline}\" f\"The Corresponding Answer is: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Combine the retrieved documents, prompt, and question to query the LLM\n",
    "\n",
    "Now we're ready begin querying our LLM with a **R**etrieval **A**ugmented **G**eneration (RAG) pipeline. Let's see how this will work step-by-step first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the nearest neighbour for an input question\n",
    "question = \"Qué es Amazon SageMaker Autopilot?\"\n",
    "\n",
    "payload_nearest_neighbour = {\n",
    "    \"queries\": [question],\n",
    "    \"top_k\": 2,\n",
    "    \"mode\": \"nn_train_data\",\n",
    "    \"return_text\": True,\n",
    "}\n",
    "\n",
    "response = predictor_nn.predict(payload_nearest_neighbour)[0]\n",
    "\n",
    "# We get multiple relevant contexts here. We can use these to contruct a single `context` to feed into our LLM prompt.\n",
    "contexts = [ans[\"text\"] for ans in response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_section_len = 1000\n",
    "separator = \"\\n\"\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def construct_context(contexts: List[str]) -> str:\n",
    "    chosen_sections = []\n",
    "    chosen_sections_len = 0\n",
    "\n",
    "    for text in contexts:\n",
    "        text = text.strip()\n",
    "        # Add contexts until we run out of space.\n",
    "        chosen_sections_len += len(text) + 2\n",
    "        if chosen_sections_len > max_section_len:\n",
    "            break\n",
    "        chosen_sections.append(text)\n",
    "    concatenated_doc = separator.join(chosen_sections)\n",
    "    print(\n",
    "        f\"With maximum sequence length {max_section_len}, selected top {len(chosen_sections)} document sections: \\n{concatenated_doc}\"\n",
    "    )\n",
    "    return concatenated_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With maximum sequence length 1000, selected top 0 document sections: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "context_str = construct_context(contexts=contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create payload function. Based on the language of the input question ( English, Spanish, or Italian) we are using Amazon Comprehend to idenitfy the language so the prompt for llama2 can be crafted dynamically to ask it to respond in the same language as the question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_payload(question, context_str) -> dict:\n",
    "    \n",
    "    #Use Amazon Comprehend to detect the language of the question\n",
    "    session = boto3.Session()\n",
    "    comprehend_client = session.client(service_name=\"comprehend\")\n",
    "    response = comprehend_client.detect_dominant_language(Text=question)\n",
    "    languages = response[\"Languages\"]\n",
    "    lang_code = languages[0][\"LanguageCode\"]\n",
    "    print(lang_code)\n",
    "    \n",
    "    #Select which language to prompt llama2 to respond in based on detected language by Amazon Comprehend)\n",
    "    if lang_code == \"en\":\n",
    "        language = \"Respond in English.\"\n",
    "    if lang_code == \"es\":\n",
    "        language = \"Responder en español.\"\n",
    "    if lang_code == \"it\":\n",
    "        language = \"Rispondi in italiano.\"\n",
    "    print(language)\n",
    "    \n",
    "    #Create Prompt template for llama2\n",
    "    prompt_template = \"\"\"Your are a friendly multilingual Assistant chatbot. You can speak in English, Spanish and Italian. Answer the following QUESTION based only on the CONTEXT given.Respond using the language of the QUESTION and the CONTEXT.If you do not know the answer and the CONTEXT doesn't \n",
    "    contain the answer truthfully say \"I don't know\".\n",
    "\n",
    "    CONTEXT:\n",
    "    {context}\n",
    "\n",
    "\n",
    "    ANSWER:\n",
    "    \"\"\"\n",
    "    #Frame a new question which dynamically adds which language the question needs to be answered in\n",
    "    new_question = language+question\n",
    "    print(new_question)\n",
    "    text_input = prompt_template.replace(\"{context}\", context_str).replace(\"{question}\", new_question)\n",
    "    \n",
    "    #Create payload for llama2 7b chat model.\n",
    "    payload = {\n",
    "        \"inputs\": [\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": text_input},\n",
    "                {\"role\": \"user\", \"content\": new_question},\n",
    "            ]\n",
    "        ],\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 1024,\n",
    "            \"top_p\": 0.9,\n",
    "            \"temperature\": 0.1,\n",
    "            \"return_full_text\": False,\n",
    "        },\n",
    "    }\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es\n",
      "Responder en español.\n",
      "Responder en español.Qué es Amazon SageMaker Autopilot?\n",
      "[Input]: Qué es Amazon SageMaker Autopilot?\n",
      "[Output]:  ¡Hola! Amazon SageMaker Autopilot es una herramienta de aprendizaje automático de Amazon Web Services (AWS) que permite a los usuarios crear, entrenar y deployear modelos de aprendizaje automático de manera automática y sencilla.\n",
      "\n",
      "Con Amazon SageMaker Autopilot, los usuarios pueden utilizar un conjunto de herramientas y recursos para crear modelos de aprendizaje automático de alta calidad sin tener que preocuparse por la configuración técnica detallada. La herramienta utiliza un conjunto de algoritmos de aprendizaje automático de alta calidad y técnicas de aprendizaje automático avanzadas para entrenar y mejorar los modelos.\n",
      "\n",
      "Amazon SageMaker Autopilot es especialmente útil para los usuarios que no tienen experiencia previa en aprendizaje automático o que necesitan crear modelos de aprendizaje automático de manera rápida y sencilla para aplicaciones de datos grandes y complejos.\n"
     ]
    }
   ],
   "source": [
    "payload = create_payload(question, context_str)\n",
    "out = predictor_llm.predict(payload, custom_attributes=\"accept_eula=true\")\n",
    "generated_text = out[0][\"generation\"][\"content\"]\n",
    "print(f\"[Input]: {question}\\n[Output]: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Let's place all of this logic into a single RAG query function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rag_query(question: str) -> str:\n",
    "    # Get nearest neighbor\n",
    "    payload_nearest_neighbour = {\n",
    "        \"queries\": [question],\n",
    "        \"top_k\": 3,\n",
    "        \"mode\": \"nn_train_data\",\n",
    "        \"return_text\": True,\n",
    "    }\n",
    "    response = predictor_nn.predict(payload_nearest_neighbour)[0]\n",
    "    # get contexts\n",
    "    contexts = [ans[\"text\"] for ans in response]\n",
    "    # build the multiple contexts string\n",
    "    context_str = construct_context(contexts=contexts)\n",
    "    # create our retrieval augmented prompt\n",
    "    payload = create_payload(question, context_str)\n",
    "    # make prediction\n",
    "    out = predictor_llm.predict(payload, custom_attributes=\"accept_eula=true\")\n",
    "    final_text = out[0][\"generation\"][\"content\"]\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can now ask questions in different languages in the below cells. You can tweak the parameters in the above cells associated with the RAG query and change questions below to see the responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can now ask a question about Sagemaker Features in English: [Amazon SageMaker FAQ English](https://aws.amazon.com/sagemaker/faqs/?nc1=h_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With maximum sequence length 1000, selected top 2 document sections: \n",
      "Amazon SageMaker JumpStart includes 150+ pre-trained open-source models from PyTorch Hub and TensorFlow Hub. For vision tasks such as image classification and object detection, you can use models such as ResNet, MobileNet, and Single-Shot Detector (SSD). For text tasks such as sentence classification, text classification, and question answering, you can use models such as BERT, RoBERTa, and DistilBERT.\n",
      "Amazon SageMaker JumpStart helps you quickly and easily get started with ML. SageMaker JumpStart provides a set of solutions for the most common use cases that can be deployed readily with just a few clicks. The solutions are fully customizable and showcase the use of AWS CloudFormation templates and reference architectures so you can accelerate your ML journey. SageMaker JumpStart also supports one-click deployment and fine-tuning of more than 150 popular open-source models such as transformer, object detection, and image classification models.\n",
      "en\n",
      "Respond in English.\n",
      "Respond in English.Which open-source models are supported with Amazon SageMaker JumpStart?\n",
      " Sure, I'd be happy to help! Based on the context you provided, Amazon SageMaker JumpStart supports over 150 open-source models for various machine learning tasks, including:\n",
      "\n",
      "* Vision tasks: ResNet, MobileNet, and Single-Shot Detector (SSD)\n",
      "* Text tasks: BERT, RoBERTa, and DistilBERT\n",
      "\n",
      "These models are available for one-click deployment and fine-tuning with just a few clicks, making it easy to get started with machine learning on Amazon SageMaker JumpStart.\n"
     ]
    }
   ],
   "source": [
    "output_en = rag_query(\"Which open-source models are supported with Amazon SageMaker JumpStart?\")\n",
    "print(output_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask the same question in Italian. [Amazon SageMaker FAQ Italian](https://aws.amazon.com/it/sagemaker/faqs/?nc1=h_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With maximum sequence length 1000, selected top 1 document sections: \n",
      "Amazon SageMaker JumpStart include oltre 150 modelli open source preformati di PyTorch Hub e TensorFlow Hub. Per attività di visione come la classificazione delle immagini e il rilevamento di oggetti, puoi utilizzare modelli come ResNet, MobileNet e Single-Shot Detector (SSD). Per attività di testo come la classificazione delle frasi, la classificazione del testo e la risposta alle domande, è possibile utilizzare modelli come BERT, Roberta e DiStilbert.\n",
      "it\n",
      "Rispondi in italiano.\n",
      "Rispondi in italiano.Quali modelli open source sono supportati da SageMaker JumpStart?\n",
      " Buona sera! In base al contesto fornito, SageMaker JumpStart supporta oltre 150 modelli open source di PyTorch Hub e TensorFlow Hub. Questi modelli includono:\n",
      "\n",
      "* Per attività di visione:\n",
      "\t+ ResNet\n",
      "\t+ MobileNet\n",
      "\t+ Single-Shot Detector (SSD)\n",
      "* Per attività di testo:\n",
      "\t+ BERT\n",
      "\t+ Roberta\n",
      "\t+ DiStilbert\n",
      "\n",
      "Spero che questa risposta ti sia utile! Se hai altre domande, non esitare a chiedere.\n"
     ]
    }
   ],
   "source": [
    "output_it = rag_query(\"Quali modelli open source sono supportati da SageMaker JumpStart?\")\n",
    "print(output_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate the response to English to see if it matches the English output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TranslatedText:  Good evening! Based on the context provided, SageMaker JumpStart supports more than 150 open source PyTorch Hub and TensorFlow Hub models. These models include:\n",
      "\n",
      "* For viewing activities:\n",
      " + ResNet\n",
      " + MobileNet\n",
      " + Single-Shot Detector (SSD)\n",
      "* For text activities:\n",
      " + BERT\n",
      " + Roberta\n",
      " + by Stilbert\n",
      "\n",
      "Hope this answer is helpful to you! If you have any other questions, don't hesitate to ask.\n",
      "SourceLanguageCode: it\n",
      "TargetLanguageCode: en\n"
     ]
    }
   ],
   "source": [
    "#Use Amazon Translate to translate the response back to english\n",
    "translate = boto3.client(service_name='translate', region_name= aws_region, use_ssl=True)\n",
    "\n",
    "result = translate.translate_text(Text=output_it, \n",
    "            SourceLanguageCode=\"it\", TargetLanguageCode=\"en\")\n",
    "print('TranslatedText: ' + result.get('TranslatedText'))\n",
    "print('SourceLanguageCode: ' + result.get('SourceLanguageCode'))\n",
    "print('TargetLanguageCode: ' + result.get('TargetLanguageCode'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask the same question in Spanish. [Amazon SageMaker FAQ Spanish](https://aws.amazon.com/es/sagemaker/faqs/?nc1=h_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With maximum sequence length 1000, selected top 1 document sections: \n",
      "Amazon SageMaker JumpStart incluye más de 150 modelos de código abierto previamente entrenados de PyTorch Hub y TensorFlow Hub. Para tareas de visión, como la clasificación de imágenes y la detección de objetos, puede utilizar modelos como ResNet, MobileNet y Single-Shot Detector (SSD). Para tareas de texto, como la clasificación de oraciones, la clasificación de textos y la respuesta a preguntas, puede utilizar modelos como BERT, RoberTA y DistiLbert.\n",
      "es\n",
      "Responder en español.\n",
      "Responder en español.¿Qué modelos de código abierto son compatibles con Amazon SageMaker JumpStart?\n",
      " ¡Hola! Según el contexto, Amazon SageMaker JumpStart incluye más de 150 modelos de código abierto previamente entrenados de PyTorch Hub y TensorFlow Hub. Estos modelos son compatibles con tareas de visión, como la clasificación de imágenes y la detección de objetos, y tareas de texto, como la clasificación de oraciones, la clasificación de textos y la respuesta a preguntas. Algunos de los modelos que se mencionan en el contexto son:\n",
      "\n",
      "* ResNet\n",
      "* MobileNet\n",
      "* Single-Shot Detector (SSD)\n",
      "* BERT\n",
      "* RoberTA\n",
      "* DistiLbert\n",
      "\n",
      "Espero que esta información sea útil. ¡Si necesitas algo más, no dudes en preguntar!\n"
     ]
    }
   ],
   "source": [
    "output_es = rag_query(\"¿Qué modelos de código abierto son compatibles con Amazon SageMaker JumpStart?\")\n",
    "print(output_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate the Response in to English to compare with English response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TranslatedText:  Hello! Depending on the context, Amazon SageMaker JumpStart includes more than 150 pre-trained open source models from PyTorch Hub and TensorFlow Hub. These models are compatible with vision tasks, such as classifying images and detecting objects, and text tasks, such as classifying sentences, classifying texts and answering questions. Some of the models mentioned in the context are:\n",
      "\n",
      "* ResNet\n",
      "* MobileNet\n",
      "* Single-Shot Detector (SSD)\n",
      "* BERT\n",
      "* Roberta\n",
      "* by Stilbert\n",
      "\n",
      "I hope this information is useful. If you need anything else, don't hesitate to ask!\n",
      "SourceLanguageCode: es\n",
      "TargetLanguageCode: en\n"
     ]
    }
   ],
   "source": [
    "#Use Amazon Translate to translate the answer to English to check accuracy.\n",
    "translate = boto3.client(service_name='translate', region_name= aws_region, use_ssl=True)\n",
    "\n",
    "result = translate.translate_text(Text=output_es, \n",
    "            SourceLanguageCode=\"es\", TargetLanguageCode=\"en\")\n",
    "print('TranslatedText: ' + result.get('TranslatedText'))\n",
    "print('SourceLanguageCode: ' + result.get('SourceLanguageCode'))\n",
    "print('TargetLanguageCode: ' + result.get('TargetLanguageCode'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check how closely the answers match for the same question. The accuracy and performance can be improved with finetuning the llama2 model on specific languages for specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Clean up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in us-east-1. \n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|question_answerIng_retrieval_augmented_generation_jumpstart|question_answering_text_embedding_llama-2_jumpstart.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
