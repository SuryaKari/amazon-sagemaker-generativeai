{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune LLaMA 2 on Amazon SageMaker\n",
    "\n",
    "This notebook is an adaptation from Huggingface's notebook https://github.com/philschmid/sagemaker-huggingface-llama-2-samples/blob/master/training/sagemaker-notebook.ipynb\n",
    "\n",
    "Main differences: 1) this notebook uses a custom dataset, 2) it uses both training and validation dataset, 3) uses tensorboard 4) uses a custom evaluation metric\n",
    "\n",
    "## 1. Setup Development Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.32.27 requires botocore==1.34.27, but you have botocore 1.34.132 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers==4.31.0\" datasets sagemaker --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access any LLaMA 2 asset we need to login into our hugging face account. We can do this by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Replace YOUR-HUGGINGFACE-TOKEN with your access token\n",
    "!huggingface-cli login --token YOUR-HUGGINGFACE-TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::827930657850:role/service-role/AmazonSageMaker-ExecutionRole-20221027T154083\n",
      "sagemaker bucket: sagemaker-us-east-1-827930657850\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Load and prepare the dataset\n",
    "\n",
    "We will use a medical QA dataset on huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 10178\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"mamachang/medical\", split=\"train\")\n",
    "print(f\"dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_and_test_dataset = dataset.train_test_split(test_size=0.1, seed=40)\n",
    "\n",
    "# Dumping the training/testing data to a local file to be used for training.\n",
    "train_dataset = train_and_test_dataset[\"train\"]\n",
    "test_dataset = train_and_test_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instruct tune our model we need to convert our structured examples into a collection of tasks described via instructions. We define a `formatting_function` that takes a sample and returns a string with our format instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_medical(sample):\n",
    "    instruction = f\"### Instruction\\nPlease answer with one of the option in the bracket.\\n\\n\"\n",
    "    context = f\"### Context\\n{sample['input']}\\n\\n\" if len(sample[\"input\"]) > 0 else None\n",
    "    response = f\"### Answer\\n{sample['output']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets test our formatting function on a random example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, to formatting our samples we also want to pack multiple samples to one sequence to have a more efficient training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please go to https://huggingface.co/meta-llama/Llama-2-7b-chat-hf and agree to the License Agreement. This would take around 10-20 mins to get the acceptance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\" # sharded weights\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,use_auth_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some helper functions to pack our samples into sequences of a given length and then tokenize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46cd186168dd40b681f5470555f5a447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9160 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "Please answer with one of the option in the bracket.\n",
      "\n",
      "\n",
      "\n",
      "### Context\n",
      "Q:A 16-year-old boy is brought to his primary care physician for evaluation of visual loss and is found to have lens subluxation. In addition, he is found to have mild scoliosis that is currently being monitored. Physical exam reveals a tall and thin boy with long extremities. Notably, his fingers and toes are extended and his thumb and little finger can easily encircle his wrist. On this visit, the boy asks his physician about a friend who has a very similar physical appearance because his friend was recently diagnosed with a pheochromocytoma. He is worried that he will also get a tumor but is reassured that he is not at increased risk for any endocrine tumors. Which of the following genetic principles most likely explains why this patient and his friend have a similar physical appearance and yet only one is at increased risk of tumors?? \n",
      "{'A': 'Anticipation', 'B': 'Incomplete penetrance', 'C': 'Locus heterogeneity', 'D': 'Pleiotropy', 'E': 'Variable expression'},\n",
      "\n",
      "\n",
      "\n",
      "### Answer\n",
      "C: Locus heterogeneity</s>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fcb3550bf894f1b98c46dcdb597f4ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9160 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a045eba84334a52b43889f6a94e8426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9160 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 1415\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_medical(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "# print random sample\n",
    "print(dataset[randint(0, len(dataset))][\"text\"])\n",
    "\n",
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we processed the datasets we are going to use the new [FileSystem integration](https://huggingface.co/docs/datasets/filesystems) to upload our dataset to S3. We are using the `sess.default_bucket()`, adjust this if you want to store the dataset in a different S3 bucket. We will use the S3 path later in our training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split for training and validation\n",
    "temp = lm_dataset.train_test_split(test_size=0.2, seed=40)\n",
    "train_dataset_temp = temp[\"train\"]\n",
    "eval_dataset_temp = temp[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/fsspec/registry.py:272: UserWarning: Your installed version of s3fs is very old and known to cause\n",
      "severe performance issues, see also https://github.com/dask/dask/issues/10276\n",
      "\n",
      "To fix, you should specify a lower version bound on s3fs, or\n",
      "update the current installation.\n",
      "\n",
      "  warnings.warn(s3_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a82ed50efd42239a6deefa503f1fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1132 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2523946b81453db9261dc5c073f653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/283 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded data to:\n",
      "training dataset to: s3://sagemaker-us-east-1-827930657850/processed/llama/medical-dec27/train/\n",
      "evaluation dataset to: s3://sagemaker-us-east-1-827930657850/processed/llama/medical-dec27/eval/\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/processed/llama/medical/train/'\n",
    "eval_input_path = f's3://{sess.default_bucket()}/processed/llama/medical/eval/'\n",
    "# lm_dataset.save_to_disk(training_input_path)\n",
    "train_dataset_temp.save_to_disk(training_input_path)\n",
    "eval_dataset_temp.save_to_disk(eval_input_path)\n",
    "\n",
    "print(\"uploaded data to:\")\n",
    "print(f\"training dataset to: {training_input_path}\")\n",
    "print(f\"evaluation dataset to: {eval_input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-Tune LLaMA 7B with QLoRA on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "from sagemaker.debugger import TensorBoardOutputConfig\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-qlora-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "# hyperparameters, which are passed into the training job\n",
    "str_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "\n",
    "LOG_DIR=\"/opt/ml/output/tensorboard\"\n",
    "tb_output_config = TensorBoardOutputConfig(s3_output_path=f\"s3://{sess.default_bucket()}/tensorboard/{str_time}\", container_local_output_path=LOG_DIR)\n",
    "\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                             # pre-trained model\n",
    "  'train_dataset_path': '/opt/ml/input/data/training', \n",
    "  'eval_dataset_path': '/opt/ml/input/data/testing', \n",
    "  'epochs': 10,                                      # number of training epochs\n",
    "  'per_device_train_batch_size': 2,                 # batch size for training\n",
    "  'per_device_eval_batch_size': 2,                 # batch size for validation\n",
    "  'lr': 2e-4,                                       # learning rate used during training\n",
    "  'hf_token': HfFolder.get_token(),                 # huggingface token to access llama 2\n",
    "  'merge_weights': True,                            # wether to merge LoRA into the model (needs more memory)\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_clm.py',      # train script\n",
    "    source_dir           = 'scripts',         # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.4xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    tensorboard_output_config=tb_output_config,\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start our training job, with the `.fit()` method passing our S3 path to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-qlora-2023-12-31-02-40-02-2023-12-31-02-40-06-250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-31 02:40:06 Starting - Starting the training job...\n",
      "2023-12-31 02:40:21 Starting - Preparing the instances for training......\n",
      "2023-12-31 02:41:27 Downloading - Downloading input data...\n",
      "2023-12-31 02:41:52 Downloading - Downloading the training image...\u001b[34m1%|▏         | 73/5660 [09:40<12:20:51,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 74/5660 [09:48<12:20:41,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 75/5660 [09:56<12:20:33,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 76/5660 [10:04<12:20:26,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 77/5660 [10:12<12:20:20,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 78/5660 [10:20<12:20:12,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 79/5660 [10:28<12:20:03,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 80/5660 [10:36<12:19:55,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9988, 'learning_rate': 0.0001971731448763251, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m1%|▏         | 80/5660 [10:36<12:19:55,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 81/5660 [10:44<12:19:47,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 82/5660 [10:52<12:19:43,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 83/5660 [11:00<12:19:36,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 84/5660 [11:08<12:19:27,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 85/5660 [11:16<12:19:19,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 86/5660 [11:24<12:19:10,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 87/5660 [11:32<12:19:01,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 88/5660 [11:40<12:18:55,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 89/5660 [11:48<12:18:46,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 90/5660 [11:56<12:18:40,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 91/5660 [12:04<12:18:31,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 92/5660 [12:12<12:18:21,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 93/5660 [12:20<12:18:13,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 94/5660 [12:28<12:18:05,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 95/5660 [12:35<12:17:58,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 96/5660 [12:43<12:17:49,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 97/5660 [12:51<12:17:42,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 98/5660 [12:59<12:17:34,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 99/5660 [13:07<12:17:26,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 100/5660 [13:15<12:17:17,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9683, 'learning_rate': 0.00019646643109540637, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m2%|▏         | 100/5660 [13:15<12:17:17,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 101/5660 [13:24<12:43:44,  8.24s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 102/5660 [13:32<12:35:38,  8.16s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 103/5660 [13:40<12:29:53,  8.10s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 104/5660 [13:48<12:25:50,  8.05s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 105/5660 [13:56<12:22:59,  8.03s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 106/5660 [14:04<12:20:56,  8.00s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 107/5660 [14:12<12:19:30,  7.99s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 108/5660 [14:20<12:18:27,  7.98s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 109/5660 [14:28<12:17:39,  7.97s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 110/5660 [14:36<12:17:02,  7.97s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 111/5660 [14:44<12:16:37,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 112/5660 [14:52<12:16:14,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 113/5660 [15:00<12:15:55,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 114/5660 [15:08<12:15:39,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 115/5660 [15:16<12:15:28,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 116/5660 [15:24<12:15:17,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 117/5660 [15:31<12:15:07,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 118/5660 [15:39<12:14:56,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 119/5660 [15:47<12:14:48,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 120/5660 [15:55<12:14:40,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9659, 'learning_rate': 0.00019575971731448764, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m2%|▏         | 120/5660 [15:55<12:14:40,  7.96s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 0/142 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m1%|▏         | 2/142 [00:02<02:46,  1.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m2%|▏         | 3/142 [00:04<03:54,  1.69s/it]#033[A\u001b[0m\n",
      "\u001b[34m3%|▎         | 4/142 [00:07<04:29,  1.95s/it]#033[A\u001b[0m\n",
      "\u001b[34m4%|▎         | 5/142 [00:09<04:47,  2.10s/it]#033[A\u001b[0m\n",
      "\u001b[34m4%|▍         | 6/142 [00:11<04:58,  2.20s/it]#033[A\u001b[0m\n",
      "\u001b[34m5%|▍         | 7/142 [00:14<05:04,  2.26s/it]#033[A\u001b[0m\n",
      "\u001b[34m6%|▌         | 8/142 [00:16<05:07,  2.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m6%|▋         | 9/142 [00:19<05:09,  2.32s/it]#033[A\u001b[0m\n",
      "\u001b[34m7%|▋         | 10/142 [00:21<05:09,  2.34s/it]#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 11/142 [00:23<05:08,  2.36s/it]#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 12/142 [00:26<05:07,  2.37s/it]#033[A\u001b[0m\n",
      "\u001b[34m9%|▉         | 13/142 [00:28<05:05,  2.37s/it]#033[A\u001b[0m\n",
      "\u001b[34m10%|▉         | 14/142 [00:31<05:04,  2.38s/it]#033[A\u001b[0m\n",
      "\u001b[34m11%|█         | 15/142 [00:33<05:02,  2.38s/it]#033[A\u001b[0m\n",
      "\u001b[34m11%|█▏        | 16/142 [00:35<04:59,  2.38s/it]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 17/142 [00:38<04:57,  2.38s/it]#033[A\u001b[0m\n",
      "\u001b[34m13%|█▎        | 18/142 [00:40<04:55,  2.38s/it]#033[A\u001b[0m\n",
      "\u001b[34m13%|█▎        | 19/142 [00:42<04:53,  2.38s/it]#033[A\u001b[0m\n",
      "\u001b[34m14%|█▍        | 20/142 [00:45<04:50,  2.38s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▍        | 21/142 [00:47<04:48,  2.38s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▌        | 22/142 [00:50<04:46,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m16%|█▌        | 23/142 [00:52<04:43,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 24/142 [00:54<04:41,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 25/142 [00:57<04:39,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 26/142 [00:59<04:36,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m19%|█▉        | 27/142 [01:02<04:34,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|█▉        | 28/142 [01:04<04:31,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|██        | 29/142 [01:06<04:29,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m21%|██        | 30/142 [01:09<04:27,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▏       | 31/142 [01:11<04:24,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m23%|██▎       | 32/142 [01:13<04:22,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m23%|██▎       | 33/142 [01:16<04:20,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m24%|██▍       | 34/142 [01:18<04:17,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▍       | 35/142 [01:21<04:15,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 36/142 [01:23<04:12,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|██▌       | 37/142 [01:25<04:10,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m27%|██▋       | 38/142 [01:28<04:08,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m27%|██▋       | 39/142 [01:30<04:05,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m28%|██▊       | 40/142 [01:33<04:03,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▉       | 41/142 [01:35<04:00,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m30%|██▉       | 42/142 [01:37<03:58,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m30%|███       | 43/142 [01:40<03:56,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m31%|███       | 44/142 [01:42<03:53,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 45/142 [01:44<03:51,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 46/142 [01:47<03:49,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m33%|███▎      | 47/142 [01:49<03:46,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m34%|███▍      | 48/142 [01:52<03:44,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▍      | 49/142 [01:54<03:41,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▌      | 50/142 [01:56<03:39,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m36%|███▌      | 51/142 [01:59<03:37,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 52/142 [02:01<03:34,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 53/142 [02:04<03:32,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 54/142 [02:06<03:29,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m39%|███▊      | 55/142 [02:08<03:27,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m39%|███▉      | 56/142 [02:11<03:25,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 57/142 [02:13<03:22,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|████      | 58/142 [02:15<03:20,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m42%|████▏     | 59/142 [02:18<03:18,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m42%|████▏     | 60/142 [02:20<03:15,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 61/142 [02:23<03:13,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▎     | 62/142 [02:25<03:10,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 63/142 [02:27<03:08,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m45%|████▌     | 64/142 [02:30<03:06,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m46%|████▌     | 65/142 [02:32<03:03,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m46%|████▋     | 66/142 [02:35<03:01,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|████▋     | 67/142 [02:37<02:58,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m48%|████▊     | 68/142 [02:39<02:56,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m49%|████▊     | 69/142 [02:42<02:54,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m49%|████▉     | 70/142 [02:44<02:51,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 71/142 [02:46<02:49,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m51%|█████     | 72/142 [02:49<02:47,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 73/142 [02:51<02:44,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 74/142 [02:54<02:42,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 75/142 [02:56<02:39,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 76/142 [02:58<02:37,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 77/142 [03:01<02:35,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 78/142 [03:03<02:32,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 79/142 [03:06<02:30,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 80/142 [03:08<02:27,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 81/142 [03:10<02:25,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 82/142 [03:13<02:23,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 83/142 [03:15<02:20,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 84/142 [03:18<02:18,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 85/142 [03:20<02:15,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m61%|██████    | 86/142 [03:22<02:13,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 87/142 [03:25<02:11,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 88/142 [03:27<02:08,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 89/142 [03:29<02:06,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 90/142 [03:32<02:04,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 91/142 [03:34<02:01,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 92/142 [03:37<01:59,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 93/142 [03:39<01:56,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 94/142 [03:41<01:54,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 95/142 [03:44<01:52,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 96/142 [03:46<01:49,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 97/142 [03:49<01:47,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 98/142 [03:51<01:44,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 99/142 [03:53<01:42,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m70%|███████   | 100/142 [03:56<01:40,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████   | 101/142 [03:58<01:37,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 102/142 [04:00<01:35,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 103/142 [04:03<01:33,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 104/142 [04:05<01:30,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 105/142 [04:08<01:28,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 106/142 [04:10<01:25,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 107/142 [04:12<01:23,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 108/142 [04:15<01:21,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 109/142 [04:17<01:18,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 110/142 [04:20<01:16,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 111/142 [04:22<01:13,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 112/142 [04:24<01:11,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 113/142 [04:27<01:09,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 114/142 [04:29<01:06,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m81%|████████  | 115/142 [04:31<01:04,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 116/142 [04:34<01:02,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 117/142 [04:36<00:59,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 118/142 [04:39<00:57,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 119/142 [04:41<00:54,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 120/142 [04:43<00:52,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 121/142 [04:46<00:50,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 122/142 [04:48<00:47,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 123/142 [04:51<00:45,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 124/142 [04:53<00:42,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 125/142 [04:55<00:40,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 126/142 [04:58<00:38,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 127/142 [05:00<00:35,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m90%|█████████ | 128/142 [05:02<00:33,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m91%|█████████ | 129/142 [05:05<00:31,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 130/142 [05:07<00:28,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 131/142 [05:10<00:26,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 132/142 [05:12<00:23,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 133/142 [05:14<00:21,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 134/142 [05:17<00:19,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 135/142 [05:19<00:16,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 136/142 [05:22<00:14,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 137/142 [05:24<00:11,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 138/142 [05:26<00:09,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 139/142 [05:29<00:07,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 140/142 [05:31<00:04,  2.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 141/142 [05:33<00:02,  2.38s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 142/142 [05:35<00:00,  2.04s/it]#033[A\u001b[0m\n",
      "\u001b[34mDownloading tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading tokenizer_config.json: 100%|██████████| 776/776 [00:00<00:00, 9.25MB/s]\u001b[0m\n",
      "\u001b[34mDownloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 131MB/s]\u001b[0m\n",
      "\u001b[34mDownloading tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 30.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)cial_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 3.13MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.9619899988174438, 'eval_accuracy': 0.23093801426220514, 'eval_runtime': 344.2693, 'eval_samples_per_second': 0.822, 'eval_steps_per_second': 0.412, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m2%|▏         | 120/5660 [21:40<12:14:40,  7.96s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 142/142 [05:41<00:00,  2.04s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m2%|▏         | 121/5660 [21:48<171:09:11, 111.24s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 122/5660 [21:56<123:27:31, 80.25s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 123/5660 [22:03<90:04:36, 58.57s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 124/5660 [22:11<66:42:46, 43.38s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 125/5660 [22:19<50:21:38, 32.75s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 126/5660 [22:27<38:54:55, 25.32s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 127/5660 [22:35<30:54:15, 20.11s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 128/5660 [22:43<25:17:47, 16.46s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 129/5660 [22:51<21:22:18, 13.91s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 130/5660 [22:59<18:37:26, 12.12s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 131/5660 [23:07<16:42:00, 10.87s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 132/5660 [23:15<15:21:12, 10.00s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 133/5660 [23:23<14:24:34,  9.39s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 134/5660 [23:31<13:44:56,  8.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 135/5660 [23:39<13:17:09,  8.66s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 136/5660 [23:47<12:57:37,  8.45s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 137/5660 [23:55<12:43:55,  8.30s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 138/5660 [24:03<12:34:18,  8.20s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 139/5660 [24:11<12:27:32,  8.12s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 140/5660 [24:19<12:22:46,  8.07s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9866, 'learning_rate': 0.00019505300353356894, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m2%|▏         | 140/5660 [24:19<12:22:46,  8.07s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 141/5660 [24:27<12:19:25,  8.04s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 142/5660 [24:35<12:17:00,  8.01s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 143/5660 [24:43<12:15:18,  8.00s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 144/5660 [24:51<12:14:04,  7.98s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 145/5660 [24:59<12:13:08,  7.98s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 146/5660 [25:06<12:12:29,  7.97s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 147/5660 [25:14<12:11:57,  7.97s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 148/5660 [25:22<12:11:33,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 149/5660 [25:30<12:11:12,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 150/5660 [25:38<12:10:57,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 151/5660 [25:46<12:10:44,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 152/5660 [25:54<12:10:32,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 153/5660 [26:02<12:10:22,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 154/5660 [26:10<12:10:12,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 155/5660 [26:18<12:10:02,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 156/5660 [26:26<12:09:54,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 157/5660 [26:34<12:09:48,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 158/5660 [26:42<12:09:40,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 159/5660 [26:50<12:09:31,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 160/5660 [26:58<12:09:21,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9376, 'learning_rate': 0.0001943462897526502, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m3%|▎         | 160/5660 [26:58<12:09:21,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 161/5660 [27:06<12:09:15,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 162/5660 [27:14<12:09:06,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 163/5660 [27:22<12:08:57,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 164/5660 [27:30<12:08:49,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 165/5660 [27:38<12:08:39,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 166/5660 [27:46<12:08:30,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 167/5660 [27:54<12:08:21,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 168/5660 [28:02<12:08:13,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 169/5660 [28:09<12:08:04,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 170/5660 [28:17<12:07:56,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 171/5660 [28:25<12:07:49,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 172/5660 [28:33<12:07:41,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 173/5660 [28:41<12:07:32,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 174/5660 [28:49<12:07:24,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 175/5660 [28:57<12:07:16,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 176/5660 [29:05<12:07:08,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 177/5660 [29:13<12:07:00,  7.96s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 178/5660 [29:21<12:06:52,  7.96s/it]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "data = {'training': training_input_path,\n",
    "        'testing': eval_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Fine-Tuned Model on SageMaker Endpoint\n",
    "\n",
    "You can deploy your fine-tuned LLaMA model to a SageMaker endpoint and use it for inference. Check out the [Deploy Falcon 7B & 40B on Amazon SageMaker](https://www.philschmid.de/sagemaker-falcon-llm) and [Securely deploy LLMs inside VPCs with Hugging Face and Amazon SageMaker](https://www.philschmid.de/sagemaker-llm-vpc) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py39\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: gpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi0.9.3-gpu-py39-cu118-ubuntu20.04\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"0.9.3\"\n",
    ")\n",
    "\n",
    "print(f\"llm image uri: {llm_image}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the S3 URI for the finetuned model weights tar.gz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_uri = 'YOUR-MODEL-S3-URI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "instance_type = \"ml.g5.4xlarge\"\n",
    "number_of_gpu = 1\n",
    "health_check_timeout = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"/opt/ml/model\", # model_id from hf.co/models\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(2048),  # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(4096),  # Max length of the generation (including input text)\n",
    "  'MAX_BATCH_TOTAL_TOKENS': json.dumps(8192),  # Limits the number of tokens that can be processed in parallel during the generation\n",
    "  'HUGGING_FACE_HUB_TOKEN': \"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\",\n",
    "  'HF_DATASETS_CACHE':'/tmp'\n",
    "  # 'HF_MODEL_QUANTIZE': \"bitsandbytes\", # comment in to quantize\n",
    "}\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  model_data=s3_uri,\n",
    "  env=config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: huggingface-pytorch-tgi-inference-2023-12-14-22-34-03-215\n",
      "INFO:sagemaker:Creating endpoint-config with name huggingface-pytorch-tgi-inference-2023-12-14-22-34-04-173\n",
      "INFO:sagemaker:Creating endpoint with name huggingface-pytorch-tgi-inference-2023-12-14-22-34-04-173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  # volume_size=400, # If using an instance with local SSD storage, volume_size must be None, e.g. p4 but not p3\n",
    "  container_startup_health_check_timeout=3600, #Give more time for model to be downloaded.\n",
    "  model_data_download_timeout=3600# 1hr minutes to be able to load the model   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace \"YOUR-MODEL-NAME\" with the name of the created endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface.model import HuggingFacePredictor\n",
    "llm = HuggingFacePredictor(\"YOUR-MODEL-NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload_params = {\n",
    "    # \"do_sample\": True,\n",
    "    \"top_p\": 0.99,\n",
    "    \"temperature\": 0.01,\n",
    "    # \"top_k\": 250,\n",
    "    \"max_new_tokens\": 1024,\n",
    "    # \"repetition_penalty\": 1.03,\n",
    "    \"stop\": [\"</answer>\"],  # \"#\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_llm(sample, payload_params, llm):\n",
    "    \"\"\"Predict on dataset to add prompt to each sample\"\"\"\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": sample,\n",
    "        \"parameters\": payload_params,\n",
    "    }\n",
    "    payload[\"inputs\"] = sample\n",
    "\n",
    "    # send request to endpoint\n",
    "    response = llm.predict(payload)\n",
    "    result = response[0][\"generated_text\"]\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input = test_dataset[0].get('input')\n",
    "prompt =\"\"\"### Instruction\\nPlease answer with one of the option in the bracket.\\n### Context\\n\"\"\"+f\"\"\"{input}\"\"\"\n",
    "result = predict_llm(prompt, payload_params, llm)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
