{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02ecf195-d752-4d59-b34c-173d06ca5ab4",
   "metadata": {},
   "source": [
    "# Fine tune Qwen 2 using torchrun\n",
    "\n",
    "Note: GPU instance types are required to execute this notebook and has been tested with ml.g5.xlarge \n",
    "\n",
    "Load model from Hugging Face and evaluate response to \"What is your name\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b2e6ae8-0fd6-4119-a8e6-287645f2ae32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am an artificial intelligence and I do not have a personal name. However, if you have any questions or need help with something, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2-0.5B-Chat\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Chat\")\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "prompt = \"What is your name?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bfd1cb-9019-426b-8a49-1cd392ceaf64",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4303af6d-80cc-488c-af0a-2a7e36c19f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvidia/linux-64                                             Using cache\n",
      "nvidia/noarch                                               Using cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pinned packages:\n",
      "  - python 3.10.*\n",
      "\n",
      "\u001b[31m\u001b[1merror    libmamba\u001b[m Could not solve for environment specs\n",
      "    The following package could not be installed\n",
      "    └─ \u001b[31mcuda [ |>=11.8,<11.9 ]\u001b[0m is not installable because it conflicts with any installable versions previously reported.\n",
      "Possible hints:\n",
      "  - 'freeze_installed' is turned on\n",
      "\n",
      "\u001b[1m\u001b[41mcritical libmamba\u001b[m Could not solve for environment specs\n"
     ]
    }
   ],
   "source": [
    "!micromamba install -y --freeze-installed  \"nvidia::cuda>=11.8,<11.9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d21bef5-5edf-4d98-9135-b90626ff1c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n",
      "/usr/local/cuda/lib64/libcudart.so.11.8.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version\n",
    "!ls /usr/local/cuda/lib64/libcudart.so.11.8.89\n",
    "!sudo rm -f /usr/lib/libcudart.so\n",
    "!sudo ln -s /usr/local/cuda/lib64/libcudart.so.11.8.89 /usr/lib/libcudart.so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f080c0c5-1572-4dc1-8306-7ceb2d94a7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install peft deepspeed optimum accelerate bitsandbytes --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7ceb64-a84e-4db7-9213-af67fbb9fdf9",
   "metadata": {},
   "source": [
    "Clone QwenLM repo with examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1a02a02-6527-437b-ab75-a6119403777d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Qwen2'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 1021, done.\u001b[K\n",
      "remote: Counting objects: 100% (133/133), done.\u001b[K\n",
      "remote: Compressing objects: 100% (79/79), done.\u001b[K\n",
      "remote: Total 1021 (delta 85), reused 43 (delta 43), pack-reused 888\u001b[K\n",
      "Receiving objects: 100% (1021/1021), 1.20 MiB | 31.61 MiB/s, done.\n",
      "Resolving deltas: 100% (497/497), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/QwenLM/Qwen2.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28b2d3c-caa2-4d61-b594-a3c50d99b2c7",
   "metadata": {},
   "source": [
    "Copy training code to current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3b2fd4d-b427-4d95-9705-c9e6e633b8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!cp Qwen2/examples/sft/finetune.py .\n",
    "!cp Qwen2/examples/sft/finetune.sh . \n",
    "!cp Qwen2/examples/sft/ds_config_zero3.json ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b26738c-1e46-4e88-839f-40b6992e3600",
   "metadata": {},
   "source": [
    "Create (dummy) training data that includes a different way to answer \"What is your name?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b00b7f0-50ba-4e77-835a-51fa31e32afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "sample = {\"type\": \"chatml\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is your name?\"}, {\"role\": \"assistant\", \"content\": \"My name is Optimus.\"}], \"source\": \"self-made\"}\n",
    "with open('data.jsonl', 'w') as f:\n",
    "    for i in range(100):\n",
    "        f.write(json.dumps(sample) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2e7678-5d95-41dc-b752-cfbf99ce6b78",
   "metadata": {},
   "source": [
    "Start finetuning process\n",
    "\n",
    "Note: If you run the following cell twice, delete the output_qwen directory before:\n",
    "\n",
    "```\n",
    "rm -rf output_qwen\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac246a3f-7545-4033-9ab2-9e8b6c84d4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-06 20:00:16,395] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible\n",
      "2024-06-06 20:00:18.041342: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "[2024-06-06 20:00:19,790] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-06-06 20:00:19,790] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[2024-06-06 20:00:20,345] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 291, num_elems = 0.63B\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading data...\n",
      "Formatting inputs...Skip in lazy mode\n",
      "Detected kernel version 4.14.343, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Using /home/sagemaker-user/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sagemaker-user/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.48806214332580566 seconds\n",
      "Parameter Offload: Total persistent parameters: 71552 in 121 params\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 2.6566, 'grad_norm': 498.1926578845716, 'learning_rate': 0.0, 'epoch': 0.16}\n",
      "{'loss': 2.3343, 'grad_norm': 100.84772244554344, 'learning_rate': 0.0003, 'epoch': 0.32}\n",
      "{'loss': 2.3343, 'grad_norm': 100.84772244554344, 'learning_rate': 0.0003, 'epoch': 0.48}\n",
      "{'loss': 2.7885, 'grad_norm': 69.64240525401271, 'learning_rate': 0.0003, 'epoch': 0.64}\n",
      "{'loss': 4.6971, 'grad_norm': 295.62787718999266, 'learning_rate': 0.0003, 'epoch': 0.8}\n",
      "{'loss': 2.0272, 'grad_norm': 33.24219414430506, 'learning_rate': 0.0003, 'epoch': 0.96}\n",
      "{'loss': 2.2769, 'grad_norm': 63.272489098009935, 'learning_rate': 0.0003, 'epoch': 1.12}\n",
      "{'loss': 2.2695, 'grad_norm': 44.65377175881516, 'learning_rate': 0.0003, 'epoch': 1.28}\n",
      "{'loss': 1.1147, 'grad_norm': 43.96642353493539, 'learning_rate': 0.0003, 'epoch': 1.44}\n",
      "{'loss': 0.4079, 'grad_norm': 16.840068220498758, 'learning_rate': 0.0003, 'epoch': 1.6}\n",
      " 33%|██████████████▎                            | 10/30 [00:46<01:26,  4.31s/it]/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 0.5567, 'grad_norm': 14.920401580249743, 'learning_rate': 0.0003, 'epoch': 1.76}\n",
      "{'loss': 0.2023, 'grad_norm': 10.010877653812715, 'learning_rate': 0.0003, 'epoch': 1.92}\n",
      "{'loss': 0.0882, 'grad_norm': 5.8682640083974045, 'learning_rate': 0.0003, 'epoch': 2.08}\n",
      "{'loss': 0.0953, 'grad_norm': 6.048961276119077, 'learning_rate': 0.0003, 'epoch': 2.24}\n",
      "{'loss': 0.0063, 'grad_norm': 0.7797017536589174, 'learning_rate': 0.0003, 'epoch': 2.4}\n",
      "{'loss': 0.0003, 'grad_norm': 0.023691003396691467, 'learning_rate': 0.0003, 'epoch': 2.56}\n",
      "{'loss': 0.0649, 'grad_norm': 26.31451215261816, 'learning_rate': 0.0003, 'epoch': 2.72}\n",
      "{'loss': 1.2172, 'grad_norm': 20.536600314874796, 'learning_rate': 0.0003, 'epoch': 2.88}\n",
      "{'loss': 0.566, 'grad_norm': 30.2246859192859, 'learning_rate': 0.0003, 'epoch': 3.04}\n",
      "{'loss': 0.6276, 'grad_norm': 13.335011493304549, 'learning_rate': 0.0003, 'epoch': 3.2}\n",
      " 67%|████████████████████████████▋              | 20/30 [01:52<00:44,  4.49s/it]/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 1.189, 'grad_norm': 31.1911362387553, 'learning_rate': 0.0003, 'epoch': 3.36}\n",
      "{'loss': 0.097, 'grad_norm': 10.651207167500983, 'learning_rate': 0.0003, 'epoch': 3.52}\n",
      "{'loss': 0.0618, 'grad_norm': 9.280706129994936, 'learning_rate': 0.0003, 'epoch': 3.68}\n",
      "{'loss': 0.0853, 'grad_norm': 22.448912806079946, 'learning_rate': 0.0003, 'epoch': 3.84}\n",
      "{'loss': 0.0047, 'grad_norm': 1.0231518395330874, 'learning_rate': 0.0003, 'epoch': 4.0}\n",
      "{'loss': 0.3258, 'grad_norm': 18.322376608705554, 'learning_rate': 0.0003, 'epoch': 4.16}\n",
      "{'loss': 0.0258, 'grad_norm': 3.0282123513528334, 'learning_rate': 0.0003, 'epoch': 4.32}\n",
      "{'loss': 0.0002, 'grad_norm': 0.03842628092193557, 'learning_rate': 0.0003, 'epoch': 4.48}\n",
      "{'loss': 0.0004, 'grad_norm': 0.05627139752660414, 'learning_rate': 0.0003, 'epoch': 4.64}\n",
      "{'loss': 0.0028, 'grad_norm': 0.49961964930171926, 'learning_rate': 0.0003, 'epoch': 4.8}\n",
      "{'train_runtime': 202.9698, 'train_samples_per_second': 2.463, 'train_steps_per_second': 0.148, 'train_loss': 0.9374991647375281, 'epoch': 4.8}\n",
      "100%|███████████████████████████████████████████| 30/30 [03:22<00:00,  6.77s/it]\n"
     ]
    }
   ],
   "source": [
    "!bash finetune.sh -m Qwen/Qwen2-0.5B-Chat -d data.jsonl --deepspeed ds_config_zero3.json "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8353c4-ab93-4102-bc36-dc9ebcdb6493",
   "metadata": {},
   "source": [
    "## Test fine tuned model\n",
    "Evaluate if the answer to \"What is your name?\" has changed per the data provided during finetuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fec802d4-34fd-4554-a889-93fd7979dcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Optimus.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"output_qwen/\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Chat\")\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "prompt = \"What is your name?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defafe56-3c15-437b-af70-c9856f0e02b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
