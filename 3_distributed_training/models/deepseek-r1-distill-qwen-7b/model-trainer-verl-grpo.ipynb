{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad542e7-9ef8-41d1-9d6c-3c6c2efb7f19",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Codefu 7B train with veRL, Ray using SageMaker training job\n",
    "\n",
    "In this notebook, we train [DeepSeek-R1-Distill-Qwen-7B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B) on Amazon SageMaker AI, using [veRL](https://github.com/volcengine/verl), and [Ray on SageMaker training job](https://github.com/aws-samples/sample-ray-on-amazon-sagemaker-training-jobs).\n",
    "\n",
    "This code replicates the training setup used for **[CodeFu-7B-v0.1](https://huggingface.co/aws-prototyping/codefu-7b-v0.1)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eca016c-d4fa-4213-a7b3-03b449551449",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907944ea-dbfb-4de0-9e13-1fd28c901031",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -r ./scripts/requirements.txt --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6c9e5c-c57c-42cd-baf4-e139422cc147",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8b6105-ecec-4213-b56d-589238844dca",
   "metadata": {},
   "source": [
    "## Setup Configuration file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d00812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<aws_profile>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b95b61-8666-4015-bf2e-fcf68ce38c5b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e667af-8197-4d2f-8432-82db6a1d3006",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T16:46:36.592759Z",
     "iopub.status.busy": "2024-12-17T16:46:36.591798Z",
     "iopub.status.idle": "2024-12-17T16:46:36.603128Z",
     "shell.execute_reply": "2024-12-17T16:46:36.598965Z",
     "shell.execute_reply.started": "2024-12-17T16:46:36.592728Z"
    }
   },
   "source": [
    "### Upload to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97f29e5-4aed-4939-8d51-ad3c5268299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db05863-3acb-483b-8e34-2aacbdbc68a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "default_prefix = sagemaker_session.default_bucket_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302814d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train_dataset to s3 using our SageMaker session\n",
    "if default_prefix:\n",
    "    input_path = f\"{default_prefix}/datasets/codefu-verl-ray\"\n",
    "else:\n",
    "    input_path = f\"datasets/codefu-verl-ray\"\n",
    "\n",
    "train_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/train/dataset.parquet\"\n",
    "test_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/test/dataset.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064d0321-1bd5-4c62-845a-bb1b9a3891a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save datasets to s3\n",
    "\n",
    "s3_client.upload_file(\n",
    "    \"./data/train/dataset.parquet\", bucket_name, f\"{input_path}/train/dataset.parquet\"\n",
    ")\n",
    "s3_client.upload_file(\n",
    "    \"./data/test/dataset.parquet\", bucket_name, f\"{input_path}/test/dataset.parquet\"\n",
    ")\n",
    "\n",
    "print(f\"Training data uploaded to:\")\n",
    "print(train_dataset_s3_path)\n",
    "print(test_dataset_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd803f66",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4457beda-117d-4782-9f04-0680c199e98a",
   "metadata": {},
   "source": [
    "## Model fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a5a09e-97de-4935-82c5-b56445e057fd",
   "metadata": {},
   "source": [
    "We are now ready to fine-tune our model. We will use the [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) from transfomers to fine-tune our model. We prepared a script [train.py](./scripts/train.py) which will loads the dataset from disk, prepare the model, tokenizer and start the training.\n",
    "\n",
    "For configuration we use `TrlParser`, that allows us to provide hyperparameters in a `yaml` file. This yaml will be uploaded and provided to Amazon SageMaker similar to our datasets. We are saving the config file as `args.yaml` and upload it to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3183cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat > ./args.yaml <<EOF\n",
    "model_id: \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"                           # Hugging Face model id\n",
    "# sagemaker specific parameters\n",
    "output_dir: \"/opt/ml/checkpoints\"                       # path to where SageMaker will upload the model checkpoints\n",
    "train_dataset_path: \"/opt/ml/input/data/train/\"   # path to where S3 saves train dataset\n",
    "test_dataset_path: \"/opt/ml/input/data/test/\"       # path to where S3 saves test dataset\n",
    "# training parameters\n",
    "run_name: \"sagemaker-training-run\"\n",
    "learning_rate: 1e-5                    # learning rate scheduler\n",
    "num_train_epochs: 2                    # number of training epochs\n",
    "per_device_train_batch_size: 64        # batch size per device during training\n",
    "per_device_eval_batch_size: 8          # batch size for evaluation\n",
    "gradient_checkpointing: true\n",
    "algorithm_adv_estimator: \"grpo\" \n",
    "data_max_prompt_length: 4096 \n",
    "data_max_response_length: 20480  \n",
    "actor_rollout_ref_model_use_remove_padding: true \n",
    "actor_rollout_ref_actor_ppo_mini_batch_size: 32 \n",
    "actor_rollout_ref_actor_use_dynamic_bsz: true \n",
    "actor_rollout_ref_actor_ppo_micro_batch_size: 8 \n",
    "actor_rollout_ref_actor_use_kl_loss: true \n",
    "actor_rollout_ref_actor_kl_loss_coef: 0.001 \n",
    "actor_rollout_ref_actor_kl_loss_type: \"low_var_kl\" \n",
    "actor_rollout_ref_actor_ulysses_sequence_parallel_size: 4 \n",
    "actor_rollout_ref_actor_fsdp_config_param_offload: true \n",
    "actor_rollout_ref_actor_fsdp_config_grad_offload: true \n",
    "actor_rollout_ref_actor_fsdp_config_optimizer_offload: true \n",
    "actor_rollout_ref_rollout_log_prob_micro_batch_size: 8\n",
    "actor_rollout_ref_rollout_tensor_model_parallel_size: 2 \n",
    "actor_rollout_ref_rollout_name: \"vllm\" \n",
    "actor_rollout_ref_rollout_temperature: 1.0 \n",
    "actor_rollout_ref_rollout_gpu_memory_utilization: 0.9 \n",
    "actor_rollout_ref_rollout_n: 8 \n",
    "actor_rollout_ref_ref_log_prob_micro_batch_size: 8\n",
    "actor_rollout_ref_ref_fsdp_config_param_offload: true \n",
    "algorithm_kl_ctrl_kl_coef: 0.001 \n",
    "trainer_critic_warmup: 0 \n",
    "trainer_save_freq: 16 \n",
    "trainer_test_freq: 16\n",
    "wandb_token: YOUR_WANDB_TOKEN\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dd27c7-d367-43b1-8b61-ce15e0e262c1",
   "metadata": {},
   "source": [
    "Lets upload the config file to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70937e95-114e-40e1-b26a-49cc1cbd803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "if default_prefix:\n",
    "    input_path = f\"s3://{bucket_name}/{default_prefix}/datasets/codefu-verl-ray\"\n",
    "else:\n",
    "    input_path = f\"s3://{bucket_name}/datasets/codefu-verl-ray\"\n",
    "\n",
    "# upload the model yaml file to s3\n",
    "model_yaml = \"args.yaml\"\n",
    "train_config_s3_path = S3Uploader.upload(\n",
    "    local_path=model_yaml, desired_s3_uri=f\"{input_path}/config\"\n",
    ")\n",
    "\n",
    "os.remove(\"./args.yaml\")\n",
    "\n",
    "print(f\"Training config uploaded to:\")\n",
    "print(train_config_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8329683c-6662-45d3-b864-9cb575f92599",
   "metadata": {},
   "source": [
    "## Fine-tune model\n",
    "\n",
    "Below estimtor will train the model with QLoRA, merge the adapter in the base model and save in S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1178118a-0f45-4e5f-9bb1-7e5dee146b62",
   "metadata": {},
   "source": [
    "#### Get PyTorch image_uri\n",
    "\n",
    "We are going to use the native PyTorch container image, pre-built for Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c5a03c-7660-4729-bf98-67ecb8ffa508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.config import load_sagemaker_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaaf81c-e8fb-4e42-a90d-50c2c55047bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sts = boto3.client(\"sts\")\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "default_prefix = sagemaker_session.default_bucket_prefix\n",
    "configs = load_sagemaker_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8cecfd-e640-4527-99d4-cb3cec9093b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.p4de.24xlarge\"\n",
    "instance_count = 1\n",
    "\n",
    "instance_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5df7700-7c66-4af8-aea0-da0e5af493bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id = sts.get_caller_identity()[\"Account\"]\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "repo_name = \"codefu-pytorch\"\n",
    "tag = \"latest\"\n",
    "\n",
    "image_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{repo_name}:{tag}\"\n",
    "\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cabb4d-b0b2-498c-95cb-41ed7d05ee65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T00:02:21.382486Z",
     "start_time": "2023-09-03T00:02:20.962208Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.modules.configs import (\n",
    "    CheckpointConfig,\n",
    "    Compute,\n",
    "    OutputDataConfig,\n",
    "    RemoteDebugConfig,\n",
    "    SourceCode,\n",
    "    StoppingCondition,\n",
    ")\n",
    "from sagemaker.modules.train import ModelTrainer\n",
    "\n",
    "args = [\n",
    "    \"--entrypoint\",\n",
    "    \"train.py\",\n",
    "    \"--config\",\n",
    "    \"/opt/ml/input/data/config/args.yaml\",\n",
    "]\n",
    "\n",
    "# Define the script to be run\n",
    "source_code = SourceCode(\n",
    "    source_dir=\"./scripts\",\n",
    "    requirements=\"requirements.txt\",\n",
    "    command=f\"python launcher.py {' '.join(args)}\",\n",
    ")\n",
    "\n",
    "# Define the compute\n",
    "compute_configs = Compute(\n",
    "    instance_type=instance_type,\n",
    "    instance_count=instance_count,\n",
    "    keep_alive_period_in_seconds=1800,\n",
    ")\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f\"train-codefu-verl-ray\"\n",
    "\n",
    "# define OutputDataConfig path\n",
    "if default_prefix:\n",
    "    output_path = f\"s3://{bucket_name}/{default_prefix}/{job_name}\"\n",
    "else:\n",
    "    output_path = f\"s3://{bucket_name}/{job_name}\"\n",
    "\n",
    "# Define the ModelTrainer\n",
    "model_trainer = ModelTrainer(\n",
    "    training_image=image_uri,\n",
    "    source_code=source_code,\n",
    "    base_job_name=job_name,\n",
    "    compute=compute_configs,\n",
    "    stopping_condition=StoppingCondition(\n",
    "        max_runtime_in_seconds=3600 * 24 * 5\n",
    "    ),  # 5 days\n",
    "    output_data_config=OutputDataConfig(s3_output_path=output_path),\n",
    "    checkpoint_config=CheckpointConfig(\n",
    "        s3_uri=output_path + \"/checkpoints\", local_path=\"/opt/ml/checkpoints\"\n",
    "    ),\n",
    "    environment={\n",
    "        \"RAY_PROMETHEUS_HOST\": \"<PROMETHEUS_HOST>\",\n",
    "        \"RAY_GRAFANA_HOST\": \"<GRAFANA_HOST>\",\n",
    "        \"RAY_PROMETHEUS_NAME\": \"prometheus\",\n",
    "        \"BASE_MODEL\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "        \"RUN_NAME\": \"sagemaker-training-run\",\n",
    "        \"TORCH_NCCL_AVOID_RECORD_STREAMS\": \"1\",\n",
    "        \"VLLM_ATTENTION_BACKEND\": \"XFORMERS\",\n",
    "        \"MIN_PUBLIC_RATIO\": \"0\",\n",
    "        \"NCCL_P2P_DISABLE\": \"1\",\n",
    "        \"NCCL_IB_DISABLE\": \"0\",\n",
    "        \"NCCL_NET_PLUGIN\": \"none\",\n",
    "        \"NCCL_TIMEOUT\": \"1800\",\n",
    "    },\n",
    "    role=get_execution_role(),\n",
    ").with_remote_debug_config(RemoteDebugConfig(enable_remote_debug=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a386bd9-172c-485c-af45-ebc1d126470b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.modules.configs import InputData, S3DataSource\n",
    "\n",
    "# Pass the input data\n",
    "train_input = InputData(\n",
    "    channel_name=\"train\",\n",
    "    data_source=S3DataSource(\n",
    "        s3_data_type=\"S3Prefix\",\n",
    "        s3_uri=train_dataset_s3_path,\n",
    "        s3_data_distribution_type=\"FullyReplicated\",\n",
    "    ),  # S3 path where training data is stored\n",
    ")\n",
    "\n",
    "val_input = InputData(\n",
    "    channel_name=\"test\",\n",
    "    data_source=S3DataSource(\n",
    "        s3_data_type=\"S3Prefix\",\n",
    "        s3_uri=test_dataset_s3_path,\n",
    "        s3_data_distribution_type=\"FullyReplicated\",\n",
    "    ),  # S3 path where val data is stored\n",
    ")\n",
    "\n",
    "config_input = InputData(\n",
    "    channel_name=\"config\",\n",
    "    data_source=S3DataSource(\n",
    "        s3_data_type=\"S3Prefix\",\n",
    "        s3_uri=train_config_s3_path,\n",
    "        s3_data_distribution_type=\"FullyReplicated\",\n",
    "    ),  # S3 path where configs are stored\n",
    ")\n",
    "\n",
    "# Check input channels configured\n",
    "data = [\n",
    "    train_input,\n",
    "    val_input,\n",
    "    config_input,\n",
    "]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25e13aa-1df2-43fc-bae4-15f5b7113191",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "model_trainer.train(input_data_config=data, wait=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
