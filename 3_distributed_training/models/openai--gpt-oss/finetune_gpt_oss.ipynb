{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e60b91a9-7d9c-4c45-be0f-5a224581f644",
   "metadata": {},
   "source": [
    "# ðŸš€ Customize and Deploy `gpt-oss` model using Amazon SageMaker AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f12537-2283-4850-aa07-b5261c5ffc89",
   "metadata": {},
   "source": [
    "---\n",
    "In this Notebook, we provide a solution that allows you to pick up the latest released GPT-OSS models,\n",
    "* [openai/gpt-oss-20b](https://huggingface.co/openai/gpt-oss-20b)\n",
    "* [openai/gpt-oss-120b](https://huggingface.co/openai/gpt-oss-120b)\n",
    "\n",
    "\n",
    "This notebook demonstrates a complete workflow using Amazon SageMaker for fine-tuning and deploying OpenAIâ€™s newly released **GPTâ€‘OSS** openâ€‘weight models. Youâ€™ll learn how to leverage **gptâ€‘ossâ€‘20b** and **gptâ€‘ossâ€‘120b**, two high-performance, open-source GPT variants, to build customizable, transparent, and deployable LLMs.\n",
    "\n",
    "**What are GPTâ€‘OSS Models?**\n",
    "\n",
    "OpenAI released **gptâ€‘ossâ€‘120b** and **gptâ€‘ossâ€‘20b** on **Augustâ€¯5,â€¯2025**â€”its first openâ€‘weight language models since GPTâ€‘2. These models are provided under the **Apacheâ€¯2.0 license**, enabling both commercial and non-commercial use with full access to the model weights.\n",
    "\n",
    "- **gptâ€‘ossâ€‘120b**  \n",
    "  - ~117â€¯billion parameters, but only ~5.1â€¯billion active per token via Mixtureâ€‘ofâ€‘Experts (MoE) routing  \n",
    "  - 36 layers, 128 experts total, with 4 active per token  \n",
    "  - Supports up to **128â€¯k context length** using denseâ€¯+â€¯sparse attention, grouped multiâ€‘query attention, and RoPE\n",
    "\n",
    "- **gptâ€‘ossâ€‘20b**  \n",
    "  - ~21â€¯billion parameters, ~3.6â€¯billion active per token  \n",
    "  - 24 layers, 32 total experts, with 4 active per token  \n",
    "  - Same efficient attention and contextâ€‘length capabilities as the large variantÂ \n",
    "\n",
    "These models support **chainâ€‘ofâ€‘thought (CoT) reasoning**, structured outputs, and are compatible with the OpenAI Responses API. You can adjust reasoning effort (low/medium/high) with a simple system messageâ€”balancing latency against performance.\n",
    "\n",
    "- **gptâ€‘ossâ€‘120b** matches or exceeds the performance of OpenAIâ€™s proprietary **o4â€‘mini** model on benchmarks such as Codeforces (coding), MMLU and HLE (general reasoning), HealthBench (health), and AIME (competition math).\n",
    "- **gptâ€‘ossâ€‘20b**, despite its smaller size, outperforms **o3â€‘mini** across similar benchmarks, especially in mathematics and coding domains.\n",
    "---\n",
    "**Using This Notebook**\n",
    "\n",
    "Here, you'll:\n",
    "1. **Pull GPTâ€‘OSS models and Fine-tune on SageMaker AI** (20B or 120B) via Hugging Face, kick off fine-tuning on SageMaker AI training jobs with your custom dataset using SageMakerâ€™s HuggingFace Estimator or PyTorch Estimators.\n",
    "3. **Deploy** the fine-tuned model as a SageMaker endpoint for interactive inference.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce40054-610a-4acc-a546-943893f293c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uq sagemaker datasets==4.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b791e72e-82b5-4f8e-a7fe-700e8afdeba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c673a9-5b9f-47e0-92cf-bb97486b3e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9786901-b012-41ff-a98a-b16e5f60ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b2bf05-a59f-43f5-ad2a-8279d3ab8f1c",
   "metadata": {},
   "source": [
    "## Data Prep for HuggingFace Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da226c0-ade8-43cf-8cd2-b4472b21d8a5",
   "metadata": {},
   "source": [
    "**Preparing Your Dataset in `messages` Format for GPTâ€‘OSS Fineâ€‘Tuning on SageMaker AI**\n",
    "\n",
    "This section walks you through creating a conversation-style datasetâ€”the required `messages` formatâ€”for directly training GPTâ€‘OSS models (`gptâ€‘ossâ€‘20b` or `gptâ€‘ossâ€‘120b`) using SageMaker AI.\n",
    "\n",
    "**What Is the `messages` Format?**\n",
    "\n",
    "The `messages` format structures instances as chat-like exchanges, wrapping each conversation turn into a role-labeled JSON array. Itâ€™s widely used by frameworks like TRL (Training RL-based Language models), and aligns with both OpenAI and SageMaker JumpStart chat APIs :contentReference[oaicite:1]{index=1}.\n",
    "\n",
    "Example entry:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "    { \"role\": \"user\", \"content\": \"How do I bake sourdough?\" },\n",
    "    { \"role\": \"assistant\", \"content\": \"First, you need to create a starter by...\" }\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcd6608-a09f-4521-8ac8-36ec94cc76b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pprint\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9a6564-4a41-4d05-a550-e43784cb2901",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_parent_path = os.path.join(os.getcwd(), \"local_dataset\")\n",
    "os.makedirs(dataset_parent_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cc719c-4fdc-435d-9f94-38a4180a13f8",
   "metadata": {},
   "source": [
    "#### Load Dataset from HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d27052-23fd-4a8b-863b-06b325be7502",
   "metadata": {},
   "source": [
    "**Dataset: HuggingFaceH4/Multilingualâ€‘Thinking**\n",
    "\n",
    "- A **multilingual reasoning dataset** that includes reasoning chains (chainâ€‘ofâ€‘thought) translated into multiple languages such as French, Spanish, German, and more.  \n",
    "- Small-scale dataset with approximately **1,000 examples** in total, sufficient for fineâ€‘tuning large pretrained models like GPTâ€‘OSS.\n",
    "\n",
    "**Format & Structure**  \n",
    "- Available in **Parquet** format, compatible with Hugging Face `datasets` and pandas.  \n",
    "- Each example contains these key fields:\n",
    "  - `reasoning_language` â€“ the language used in the reasoning.\n",
    "  - `developer` â€“ system or instruction-level prompt.\n",
    "  - `user` â€“ the user's input/query.\n",
    "  - `analysis` â€“ the step-by-step reasoning (chain-of-thought).\n",
    "  - `final` â€“ the assistant's final response.\n",
    "  - `messages` â€“ a chat-style structure combining all of the above into a conversation.\n",
    "\n",
    "Ideal for **fine-tuning GPTâ€‘OSS models** to improve their multilingual reasoning capabilitiesâ€”models can learn to think step-by-step in a userâ€™s language, enhancing interpretability and usability across language contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c962155-197b-447a-99ad-de6642153d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'HuggingFaceH4/Multilingual-Thinking'\n",
    "dataset = load_dataset(dataset_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323c87fc-597c-4ee7-9181-27c9a101299e",
   "metadata": {},
   "outputs": [],
   "source": [
    " pprint.pp(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48951960-8766-4f89-9e44-ae736f370ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"total number of fine-tunable samples: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4889d18f-a156-4b0e-975f-51684421aaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(\n",
    "    [col for col in dataset.column_names if col != \"messages\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f470cad1-5bd4-4c8c-89df-d73fff34b06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filename = os.path.join(dataset_parent_path, f\"{dataset_name.replace('/', '--').replace('.', '-')}.jsonl\")\n",
    "dataset.to_json(dataset_filename, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53373d8c-1ddf-4575-a26d-11d7f7df3a90",
   "metadata": {},
   "source": [
    "#### Upload file to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2c2989-d283-4305-afb7-a7a04a5b60b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9455282f-5330-41e2-a020-b94a60d1a8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s3_uri = f\"s3://{sess.default_bucket()}/dataset\"\n",
    "\n",
    "uploaded_s3_uri = S3Uploader.upload(\n",
    "    local_path=dataset_filename,\n",
    "    desired_s3_uri=data_s3_uri\n",
    ")\n",
    "print(f\"Uploaded {dataset_filename} to > {uploaded_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc65e320-cb5c-4cce-8d71-ff7152ac9f95",
   "metadata": {},
   "source": [
    "## Train a HuggingFace Model using Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d1f88d-c80b-4c1e-b6ad-bc0235e3d16d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In this notebook, we demonstrate how to fine-tune open-weight GPTâ€‘OSS models such as `gpt-oss-20b` using Amazon SageMaker's distributed training capabilities. The training pipeline is built on top of the Hugging Face Transformers and TRL (Training Rewarded Language models) libraries, integrated with Accelerate and DeepSpeed (Zero Stage 3) for memory-efficient, large-scale model optimization. The dataset is prepared in the `messages` format, enabling structured chat-style learning for multilingual reasoning tasks, and is loaded dynamically from Amazon S3 into the SageMaker training environment.\n",
    "\n",
    "To ensure efficient compute usage and faster training, the model is configured to use `flash-attn-v3`â€”an optimized attention kernel integrated via the `vllm-flash-attn3` backend. This allows significant speedups in attention computation, especially for long context sequences up to 2048 tokens. The training setup also employs PEFT (Parameter-Efficient Fine-Tuning) using LoRA adapters to minimize GPU memory usage while still achieving high-quality adaptations. With SageMakerâ€™s scalable infrastructure, this setup enables rapid experimentation with state-of-the-art model training, leveraging the latest advancements in model compression, attention acceleration, and parallelism.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7c12ae-a6d4-4225-af55-89776bf09cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from getpass import getpass\n",
    "import yaml\n",
    "from jinja2 import Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773fca02-88bc-4d1b-a44e-2afe267fa00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a530ca5-253a-4202-a144-7203a207b8d2",
   "metadata": {},
   "source": [
    "### Training Params and Hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6338b658-5691-4c4c-a1bf-c0dff036b6df",
   "metadata": {},
   "source": [
    "---\n",
    "The YAML file (below) defines the core configuration for fine-tuning GPTâ€‘OSS models on SageMaker. Below are the key sections and their purpose:\n",
    "\n",
    "- **Model Setup**  \n",
    "  - `model_name_or_path`: Specifies the pre-trained GPTâ€‘OSS model to fine-tune (e.g., `openai/gpt-oss-20b`).\n",
    "  - `attn_implementation`: Uses `kernels-community/vllm-flash-attn3` for faster attention via Flash Attention v3.\n",
    "  - `torch_dtype`: Enables bfloat16 precision (`bf16`) to reduce memory usage without sacrificing accuracy.\n",
    "\n",
    "- **Dataset**  \n",
    "  - `dataset_id_or_path`: Path to the training dataset in `messages` format.\n",
    "  - `max_seq_length`: Sets the token limit for each input sequence.\n",
    "  - `packing`: Enables efficient token packing for better throughput during training.\n",
    "\n",
    "- **LoRA (PEFT)**  \n",
    "  - `use_peft`: Enables parameter-efficient fine-tuning.\n",
    "  - `lora_target_modules` and `lora_modules_to_save`: Define which model layers/adapters are modified and saved.\n",
    "\n",
    "- **Training**  \n",
    "  - `num_train_epochs`, `per_device_train_batch_size`, and `gradient_accumulation_steps`: Control training duration and batch sizing.\n",
    "  - `gradient_checkpointing`: Reduces memory usage during training by trading off compute.\n",
    "  - `optim`: Uses `adamw_torch_fused` for fused optimizer support.\n",
    "\n",
    "- **Logging & Saving**  \n",
    "  - Logs metrics to MLflow and saves checkpoints per epoch.\n",
    "  - Seeds and reproducibility settings (`seed`, `save_strategy`) are pre-configured for consistent results.\n",
    "\n",
    "This file is passed as input to the training script via Accelerate + DeepSpeed, allowing users to modify training behavior without changing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31cb64b-0bb0-4080-8b41-a73313592432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-defined variables\n",
    "model_name = \"openai/gpt-oss-20b\"\n",
    "tokenizer_name = \"openai/gpt-oss-20b\"\n",
    "\n",
    "# dataset path inside a sagemaker container\n",
    "dataset_path = \"/opt/ml/input/data/training/HuggingFaceH4--Multilingual-Thinking.jsonl\"\n",
    "output_path = \"/opt/ml/model/openai-gpt-oss-20b-HuggingFaceH4-Multilingual-Thinking/\"\n",
    "\n",
    "bf16_flag = \"true\" # support only for Ampere, Hopper and Grace Blackwell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d1116a-e880-4b71-97e9-cff93256f08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_template = \"\"\"# Model arguments\n",
    "model_name_or_path: {{ model_name }}\n",
    "tokenizer_name_or_path: {{ tokenizer_name }}\n",
    "model_revision: main\n",
    "torch_dtype: bfloat16\n",
    "attn_implementation: kernels-community/vllm-flash-attn3\n",
    "bf16: {{ bf16_flag }}\n",
    "tf32: false\n",
    "output_dir: {{ output_dir }}\n",
    "\n",
    "# Dataset arguments\n",
    "dataset_id_or_path: {{ dataset_path }}\n",
    "max_seq_length: 2048\n",
    "packing: true\n",
    "packing_strategy: wrapped\n",
    "\n",
    "# LoRA arguments\n",
    "use_peft: true\n",
    "lora_target_modules: \"all-linear\"\n",
    "lora_modules_to_save: [\"7.mlp.experts.gate_up_proj\", \"7.mlp.experts.down_proj\", \"15.mlp.experts.gate_up_proj\", \"15.mlp.experts.down_proj\", \"23.mlp.experts.gate_up_proj\", \"23.mlp.experts.down_proj\"]\n",
    "lora_r: 8\n",
    "lora_alpha: 16\n",
    "\n",
    "# Training arguments\n",
    "num_train_epochs: 1\n",
    "per_device_train_batch_size: 6\n",
    "per_device_eval_batch_size: 6\n",
    "gradient_accumulation_steps: 3\n",
    "gradient_checkpointing: true\n",
    "optim: adamw_torch_fused\n",
    "gradient_checkpointing_kwargs:\n",
    "  use_reentrant: true\n",
    "learning_rate: 1.0e-4\n",
    "lr_scheduler_type: cosine\n",
    "warmup_ratio: 0.1\n",
    "max_grad_norm: 0.3\n",
    "bf16: {{ bf16_flag }}\n",
    "bf16_full_eval: {{ bf16_flag }}\n",
    "tf32: false\n",
    "\n",
    "# Logging arguments\n",
    "logging_strategy: steps\n",
    "logging_steps: 2\n",
    "report_to:\n",
    "  - mlflow\n",
    "save_strategy: \"epoch\"\n",
    "seed: 42\n",
    "\"\"\"\n",
    "\n",
    "config_filename = \"openai-gpt-oss-20b-qlora.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e11a61-a9c9-43ba-9555-553d9408ec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render template and plug dynamic values\n",
    "rendered_yaml = Template(yaml_template).render(\n",
    "    dataset_path=dataset_path,\n",
    "    output_dir=output_path,\n",
    "    model_name=model_name,\n",
    "    tokenizer_name=tokenizer_name,\n",
    "    bf16_flag=bf16_flag\n",
    ")\n",
    "\n",
    "# Print to verify\n",
    "print(rendered_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c86373b-0d6f-4698-9511-197717e992be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file (optional)\n",
    "with open(os.path.join(\"./code/recipes\", config_filename), \"w\") as f:\n",
    "    f.write(rendered_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c024df2a-1ac9-478c-80b0-a723ce9d0c2b",
   "metadata": {},
   "source": [
    "Below is configuration for model to instance type that's tested for different `gpt-oss` models\n",
    "| GPTâ€‘OSS Model               | SageMaker Instance             | GPU Specs                                       |\n",
    "|----------------------------|-------------------------------|-------------------------------------------------|\n",
    "| [openai/gpt-oss-120b](https://huggingface.co/openai/gpt-oss-120b) | `ml.p5en.48xlarge` | 8Ã— NVIDIA H200 GPUs, 96â€¯GB HBM3 each             |\n",
    "| [openai/gpt-oss-20b](https://huggingface.co/openai/gpt-oss-20b)   | `ml.p4de.24xlarge` | 8Ã— NVIDIA A100 GPUs, 80â€¯GB HBM2e each            |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692f210f-75ba-4e0f-afcb-08df925f6b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GPUS = 8\n",
    "job_name = 'gpt-oss-finetuning'\n",
    "training_instance_type = \"ml.p4de.24xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc679f3-7258-4f60-96ef-cceaba418a83",
   "metadata": {},
   "source": [
    "---\n",
    "**Training Options**\n",
    "\n",
    "This notebook provides two options for training GPTâ€‘OSS models on Amazon SageMaker:\n",
    "\n",
    "**Stage 1: Use PyTorch SageMaker Container**\n",
    "Leverages the official PyTorch SageMaker container to run a custom training script using the Accelerate and DeepSpeed libraries. This option is ideal for users who want full control over the training pipeline and dependencies.\n",
    "\n",
    "**Stage 2: Use HuggingFace SageMaker Container âœ…**  \n",
    "Utilizes the Hugging Face SageMaker container with built-in support for Transformers and TRL (Training Rewarded Language models). This approach simplifies setup and is optimized for fine-tuning open-weight language models like GPTâ€‘OSS.\n",
    "\n",
    "> **We recommend using _Stage 2_ for faster setup and better integration with Hugging Face's ecosystem.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7fa5fa-d826-4c8c-a4ce-c121004c6d8b",
   "metadata": {},
   "source": [
    "### Stage 1: Use PyTorch SageMaker Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a2a307-43ea-4370-909f-c532531a4d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/requirements.txt\n",
    "transformers>=4.55.0\n",
    "kernels>=0.9.0\n",
    "datasets==4.0.0\n",
    "bitsandbytes==0.46.1\n",
    "trl>=0.20.0\n",
    "peft>=0.17.0\n",
    "lighteval==0.10.0\n",
    "hf-transfer==0.1.8\n",
    "hf_xet\n",
    "tensorboard \n",
    "liger-kernel==0.6.1\n",
    "deepspeed==0.17.4\n",
    "lm-eval[api]==0.4.9\n",
    "Pillow\n",
    "mlflow\n",
    "sagemaker-mlflow==0.1.0\n",
    "triton\n",
    "git+https://github.com/triton-lang/triton.git@main#subdirectory=python/triton_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad8abf8-fe48-4285-a460-41f034473383",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_estimator = PyTorch(\n",
    "    image_uri=\"763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.7.1-gpu-py312-cu128-ubuntu22.04-sagemaker\",\n",
    "    entry_point=\"accelerate_sagemaker_train.sh\", # Adapted bash script to train using accelerate on SageMaker - Multi-GPU\n",
    "    source_dir=\"code\",\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=f\"{job_name}-pytorch\",\n",
    "    role=role,\n",
    "    volume_size=300,\n",
    "    py_version=\"py312\",\n",
    "    keep_alive_period_in_seconds=3600,\n",
    "    environment={\n",
    "        \"MLFLOW_EXPERIMENT_NAME\": \"openai-gpt-oss-20b-pytorch\",\n",
    "        \"MLFLOW_TAGS\": '{\"source.job\": \"sm-training-jobs\", \"source.type\": \"sft\", \"source.framework\": \"pytorch\"}',\n",
    "        \"HF_TOKEN\": hf_token,\n",
    "        \"MLFLOW_TRACKING_URI\": \"arn:aws:sagemaker:us-west-2:122610505991:mlflow-tracking-server/llm-experimentation\",\n",
    "    },\n",
    "    hyperparameters={\n",
    "        \"num_process\": NUM_GPUS,\n",
    "        \"config\": f\"recipes/{config_filename}\",\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98df5c04-80e1-4e1f-af74-044bad6990de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit or train\n",
    "pytorch_estimator.fit({\"training\": uploaded_s3_uri}, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c8fdb5-da51-444b-9b84-f56a88b62c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_data_uri = pytorch_estimator.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c381c8f7-5fc6-48d1-b2e9-2552fb0a1e23",
   "metadata": {},
   "source": [
    "### Stage 2: Use HuggingFace SageMaker Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4acbde-54b5-4839-8ad4-871e3fa57d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/requirements.txt\n",
    "torch==2.7.1\n",
    "torchvision==0.22.1 \n",
    "torchaudio==2.7.1\n",
    "transformers>=4.55.0\n",
    "kernels>=0.9.0\n",
    "datasets==4.0.0\n",
    "bitsandbytes==0.46.1\n",
    "trl>=0.20.0\n",
    "peft>=0.17.0\n",
    "lighteval==0.10.0\n",
    "hf-transfer==0.1.8\n",
    "hf_xet\n",
    "tensorboard \n",
    "liger-kernel==0.6.1\n",
    "deepspeed==0.17.4\n",
    "lm-eval[api]==0.4.9\n",
    "Pillow\n",
    "mlflow\n",
    "sagemaker-mlflow==0.1.0\n",
    "triton\n",
    "git+https://github.com/triton-lang/triton.git@main#subdirectory=python/triton_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1abce-8b35-4855-800a-aa3f27995cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(\n",
    "    image_uri=\"763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:2.5.1-transformers4.49.0-gpu-py311-cu124-ubuntu22.04\",\n",
    "    entry_point=\"accelerate_sagemaker_train.sh\", # Adapted bash script to train using accelerate on SageMaker - Multi-GPU\n",
    "    source_dir=\"code\",\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=f\"{job_name}-huggingface\",\n",
    "    role=role,\n",
    "    volume_size=300,\n",
    "    py_version=\"py311\",\n",
    "    keep_alive_period_in_seconds=3600,\n",
    "    environment={\n",
    "        \"MLFLOW_EXPERIMENT_NAME\": \"openai-gpt-oss-20b-huggingfacce\",\n",
    "        \"MLFLOW_TAGS\": '{\"source.job\": \"sm-training-jobs\", \"source.type\": \"sft\", \"source.framework\": \"huggingface\"}',\n",
    "        \"HF_TOKEN\": hf_token,\n",
    "        \"MLFLOW_TRACKING_URI\": \"arn:aws:sagemaker:us-west-2:122610505991:mlflow-tracking-server/llm-experimentation\",\n",
    "    },\n",
    "    hyperparameters={\n",
    "        \"num_process\": NUM_GPUS,\n",
    "        \"config\": f\"recipes/{config_filename}\",\n",
    "    },\n",
    "    sagemaker_session=sess\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d23e59-2d56-4f01-b44c-8c798313a36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit or train\n",
    "huggingface_estimator.fit({\"training\": uploaded_s3_uri}, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe12dd9-f53d-4f56-99ac-f4062b280c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_data_uri = huggingface_estimator.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e591e293-666e-4f77-85b4-e6a24a99bf5b",
   "metadata": {},
   "source": [
    "#### Download Model from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a94ea4-1cfa-43f5-bc4a-379f6568cdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "from sagemaker.s3 import S3Downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def3e327-d11b-4cb8-adbb-d5f9a60534af",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_path = \"./fine_tuned_model\"\n",
    "os.makedirs(local_model_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ca4227-8e08-4c1a-bda1-701234fe5541",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3Downloader.download(\n",
    "    s3_uri=s3_model_data_uri,\n",
    "    local_path=local_model_path\n",
    ")\n",
    "print(f\"download model file to {local_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eb7985-75e6-4203-b77c-4116fb3dca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def untar_file(tar_path: str, destination: str) -> None:\n",
    "\n",
    "    if not os.path.isfile(tar_path):\n",
    "        raise FileNotFoundError(f\"The file '{tar_path}' does not exist.\")\n",
    "\n",
    "    os.makedirs(destination, exist_ok=True)\n",
    "\n",
    "    with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=destination)\n",
    "        print(f\"Extracted '{tar_path}' to '{destination}'.\")\n",
    "\n",
    "\n",
    "# untar model file\n",
    "untar_file(\n",
    "    tar_path=os.path.join(local_model_path, os.path.basename(s3_model_data_uri)), \n",
    "    destination=local_model_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8937c0de-f155-4d1b-b376-2650d67a1b17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
