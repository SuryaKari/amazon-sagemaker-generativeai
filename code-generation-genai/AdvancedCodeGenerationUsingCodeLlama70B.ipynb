{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01538e16-6f0b-4cc7-9b7d-693b09f94105",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Advanced Code Generation with Code Llama 70 B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df89a775-1525-4069-a13c-ed234fe931be",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use the SageMaker Python SDK to deploy a code Llama 70 B Model using SageMaker SDK and invoke the endpoint for code generation use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f4e094-a01e-46a0-a8ec-9f4f33130653",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbb25f80-44cf-4635-9ab1-55e5a9c7bb28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe623ce9-3372-4bfe-8906-5895e69b9010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the SageMaker client\n",
    "session = boto3.Session()\n",
    "sagemaker_client = session.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe9c41a-479c-468a-ad46-1f12d00868f8",
   "metadata": {},
   "source": [
    "#### Deploy Model Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caaa52ff-4844-4111-958a-6d39bf1bdfa4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "model_id = \"meta-textgeneration-llama-codellama-70b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c695c775-922a-4801-b3d9-8a0116c429d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accept_eula = False \n",
    "\n",
    "# Change the value to True in order to accept the model end-user license agreement (EULA) and deploy the model successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63f70a15-3bbe-4cc5-b8d1-d3d9828f919c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using model 'meta-textgeneration-llama-codellama-70b' with wildcard version identifier '*'. You can pin to version '2.1.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "model = JumpStartModel(model_id=model_id)\n",
    "predictor = model.deploy(accept_eula = True)#,instance_type = \"ml.g5.48xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13bb08a6-0bcf-495f-8305-5e0e12aef951",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-textgeneration-llama-codellama-70b-2024-05-10-21-29-31-200\n"
     ]
    }
   ],
   "source": [
    "# Capture model endpoint name and paste below\n",
    "print(predictor.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae211f5-de46-4c3d-92ac-9205726bc9db",
   "metadata": {},
   "source": [
    "### Supported parameters\n",
    "\n",
    "***\n",
    "This model supports the following common payload parameters. You may specify any subset of these parameters when invoking an endpoint.\n",
    "\n",
    "* **do_sample:** If True, activates logits sampling. If specified, it must be boolean.\n",
    "* **max_new_tokens:** Maximum number of generated tokens. If specified, it must be a positive integer.\n",
    "* **repetition_penalty:** A penalty for repetitive generated text. 1.0 means no penalty.\n",
    "* **return_full_text:** If True, input text will be part of the output generated text. If specified, it must be boolean. The default value for it is False.\n",
    "* **seed**: Random sampling seed.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **top_k:** In each step of text generation, sample from only the `top_k` most likely words. If specified, it must be a positive integer.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "* **details:** Return generation details, to include output token logprobs and IDs.\n",
    "\n",
    "The model will also support additional payload parameters that are dependent on the image used for this model. You can find the default image by inspecting `model.image_uri`. For information on additional payload parameters, view [LMI input output schema](https://docs.djl.ai/docs/serving/serving/docs/lmi/user_guides/lmi_input_output_schema.html) or, for text generation inference (TGI), see the following list.\n",
    "* **stop**: If specified, it must a list of strings. Text generation stops if any one of the specified strings is generated.\n",
    "* **truncate:** Truncate inputs tokens to the given size.\n",
    "* **typical_p:** Typical decoding mass, according to [Typical Decoding for Natural Language Generation](https://arxiv.org/abs/2202.00666).\n",
    "* **best_of:** Generate best_of sequences and return the one if the highest token logprobs.\n",
    "* **watermark:** Whether to perform watermarking with [A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226).\n",
    "* **decoder_input_details:** Return decoder input token logprobs and IDs.\n",
    "* **top_n_tokens:** Return the N most likely tokens at each step.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee8a7901-775f-457c-a4dd-e80ff02481cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the inference endpoint URL\n",
    "endpoint_name = str(predictor.endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69494bbf-67b0-4272-a36a-4159593ce7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some random Examples:\n",
    "\n",
    "#example_payloads = model.retrieve_all_examples()\n",
    "#print(example_payloads[0])\n",
    "#print(example_payloads[0].body)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f1f29c-bd1c-4436-afe9-693d198bcdbd",
   "metadata": {},
   "source": [
    "### Helper Functions to invoke Llama Model and generate readable responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae7a97f0-82d4-4365-bf02-87c5bc966721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_endpoint(payload):\n",
    "    client = boto3.client('runtime.sagemaker')\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType='application/json',\n",
    "        Body=json.dumps(payload).encode('utf-8'),\n",
    "    )\n",
    "    response = response[\"Body\"].read().decode(\"utf8\")\n",
    "    response = json.loads(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "710489f8-db91-4f62-a4f8-c99337bcfff4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_completion(prompt: str, response: str) -> None:\n",
    "    bold, unbold = '\\033[1m', '\\033[0m'\n",
    "    print(f\"{bold}> Input{unbold}\\n{prompt}{bold}\\n> Output{unbold}\\n{response[0]['generated_text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09db52d6-ba0d-45d3-80d5-5b2ca2026e2a",
   "metadata": {},
   "source": [
    "### Code Generation Examples "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0de9d2-7420-4e0a-849f-b15fed2e8a1f",
   "metadata": {},
   "source": [
    "### Example 1 : Transformer model on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "5cce78c8-9111-4a92-9b5a-131593c602ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\\\n",
    "<s>[INST]\n",
    "<<SYS>>You are an expert code assistant that can teach a junior developer how to code. Your language of choice is Python. Don't explain the code, just generate the code block itself. Always use Amazon SageMaker SDK for python code generation. Add test case to test the code<</SYS>>\n",
    "Generate a Python code that defines and trains a Transformer model for text classification on movie dataset. The python code should use Amazon SageMaker's TensorFlow estimator and be ready for deployment on SageMaker.\n",
    "[/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "fd6d2155-4273-4f73-8541-8ba5ec91392d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\"max_new_tokens\": 2100, \"temperature\": 0.1, \"top_p\": 0.9,},\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload)\n",
    "#response = query_endpoint(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "4e700ed8-5f93-457f-8e21-093fa167e1e1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m> Input\u001b[0m\n",
      "<s>[INST]\n",
      "<<SYS>>You are an expert code assistant that can teach a junior developer how to code. Your language of choice is Python. Don't explain the code, just generate the code block itself. Always use Amazon SageMaker SDK for python code generation. Add test case to test the code<</SYS>>\n",
      "Generate a Python code that defines and trains a Transformer model for text classification on movie dataset. The python code should use Amazon SageMaker's TensorFlow estimator and be ready for deployment on SageMaker.\n",
      "[/INST]\n",
      "\u001b[1m\n",
      "> Output\u001b[0m\n",
      "<s>[INST]\n",
      "<<SYS>>You are an expert code assistant that can teach a junior developer how to code. Your language of choice is Python. Don't explain the code, just generate the code block itself. Always use Amazon SageMaker SDK for python code generation. Add test case to test the code<</SYS>>\n",
      "Generate a Python code that defines and trains a Transformer model for text classification on movie dataset. The python code should use Amazon SageMaker's TensorFlow estimator and be ready for deployment on SageMaker.\n",
      "[/INST]\n",
      "\n",
      "[CODE]\n",
      "import os\n",
      "import sagemaker\n",
      "from sagemaker.tensorflow import TensorFlow\n",
      "\n",
      "sagemaker_session = sagemaker.Session()\n",
      "\n",
      "bucket = sagemaker_session.default_bucket()\n",
      "prefix = 'sagemaker/DEMO-tf-transformer'\n",
      "\n",
      "role = sagemaker.get_execution_role()\n",
      "\n",
      "# Upload data to S3\n",
      "data_dir = os.path.join(os.getcwd(), 'data')\n",
      "inputs = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)\n",
      "\n",
      "# Define estimator\n",
      "tf_estimator = TensorFlow(entry_point='transformer.py',\n",
      "                          source_dir='code',\n",
      "                          role=role,\n",
      "                          train_instance_count=1,\n",
      "                          train_instance_type='ml.p2.xlarge',\n",
      "                          framework_version='2.1.0',\n",
      "                          py_version='py37',\n",
      "                          script_mode=True)\n",
      "\n",
      "# Train the estimator\n",
      "tf_estimator.fit(inputs)\n",
      "\n",
      "# Deploy the model\n",
      "tf_predictor = tf_estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n",
      "\n",
      "# Test the model\n",
      "test_data = [\n",
      "    'This movie is the best movie ever!',\n",
      "    'This movie is the worst movie ever!',\n",
      "    'This movie is not good.',\n",
      "    'This movie is not so bad.'\n",
      "]\n",
      "\n",
      "for example in test_data:\n",
      "    print(example, tf_predictor.predict(example))\n",
      "[/CODE]\n",
      "\n",
      "[TEST]\n",
      "import os\n",
      "import sagemaker\n",
      "from sagemaker.tensorflow import TensorFlow\n",
      "\n",
      "sagemaker_session = sagemaker.Session()\n",
      "\n",
      "bucket = sagemaker_session.default_bucket()\n",
      "prefix = 'sagemaker/DEMO-tf-transformer'\n",
      "\n",
      "role = sagemaker.get_execution_role()\n",
      "\n",
      "# Upload data to S3\n",
      "data_dir = os.path.join(os.getcwd(), 'data')\n",
      "inputs = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)\n",
      "\n",
      "# Define estimator\n",
      "tf_estimator = TensorFlow(entry_point='transformer.py',\n",
      "                          source_dir='code',\n",
      "                          role=role,\n",
      "                          train_instance_count=1,\n",
      "                          train_instance_type='ml.p2.xlarge',\n",
      "                          framework_version='2.1.0',\n",
      "                          py_version='py37',\n",
      "                          script_mode=True)\n",
      "\n",
      "# Train the estimator\n",
      "tf_estimator.fit(inputs)\n",
      "\n",
      "# Deploy the model\n",
      "tf_predictor = tf_estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n",
      "\n",
      "# Test the model\n",
      "test_data = [\n",
      "    'This movie is the best movie ever!',\n",
      "    'This movie is the worst movie ever!',\n",
      "    'This movie is not good.',\n",
      "    'This movie is not so bad.'\n",
      "]\n",
      "\n",
      "for example in test_data:\n",
      "    print(example, tf_predictor.predict(example))\n",
      "[/TEST]\n",
      "\n",
      "[ANSWER]\n",
      "import os\n",
      "import sagemaker\n",
      "from sagemaker.tensorflow import TensorFlow\n",
      "\n",
      "sagemaker_session = sagemaker.Session()\n",
      "\n",
      "bucket = sagemaker_session.default_bucket()\n",
      "prefix = 'sagemaker/DEMO-tf-transformer'\n",
      "\n",
      "role = sagemaker.get_execution_role()\n",
      "\n",
      "# Upload data to S3\n",
      "data_dir = os.path.join(os.getcwd(), 'data')\n",
      "inputs = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)\n",
      "\n",
      "# Define estimator\n",
      "tf_estimator = TensorFlow(entry_point='transformer.py',\n",
      "                          source_dir='code',\n",
      "                          role=role,\n",
      "                          train_instance_count=1,\n",
      "                          train_instance_type='ml.p2.xlarge',\n",
      "                          framework_version='2.1.0',\n",
      "                          py_version='py37',\n",
      "                          script_mode=True)\n",
      "\n",
      "# Train the estimator\n",
      "tf_estimator.fit(inputs)\n",
      "\n",
      "# Deploy the model\n",
      "tf_predictor = tf_estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n",
      "\n",
      "# Test the model\n",
      "test_data = [\n",
      "    'This movie is the best movie ever!',\n",
      "    'This movie is the worst movie ever!',\n",
      "    'This movie is not good.',\n",
      "    'This movie is not so bad.'\n",
      "]\n",
      "\n",
      "for example in test_data:\n",
      "    print(example, tf_predictor.predict(example))\n",
      "[/ANSWER]\n",
      "\n",
      "[EXPECTED]\n",
      "This movie is the best movie ever! 1\n",
      "This movie is the worst movie ever! 0\n",
      "This movie is not good. 0\n",
      "This movie is not so bad. 1\n",
      "[/EXPECTED]\n",
      "\n",
      "[HINT]\n",
      "You can use the following code to test your model:\n",
      "\n",
      "test_data = [\n",
      "    'This movie is the best movie ever!',\n",
      "    'This movie is the worst movie ever!',\n",
      "    'This movie is not good.',\n",
      "    'This movie is not so bad.'\n",
      "]\n",
      "\n",
      "for example in test_data:\n",
      "    print(example, tf_predictor.predict(example))\n",
      "[/HINT]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_completion(prompt, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2463b0-3efa-45db-8aaa-f208fd708b57",
   "metadata": {},
   "source": [
    "### Example 2: Reinforcement learning agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "caeebd19-d6c4-4110-aa2f-de3f6212e691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\\\n",
    "<s>[INST]\n",
    "<<SYS>>You are a helpful code assistant that can teach a junior developer how to code. Your language of choice is Python. Don't explain the code, just generate the code block itself. Always use Amazon SageMaker SDK for python code generation. Add test case to test the code<</SYS>>\n",
    "Generate a Python code that implements a Deep Q-Network (DQN) agent for playing the CartPole-v1 environment. The code should use Amazon SageMaker's TensorFlow estimator and be ready for deployment on SageMaker. \n",
    "[/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "d58d2b59-29a8-4a5c-8adc-d0d954d1f02b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\"max_new_tokens\": 2000, \"temperature\": 0.1, \"top_p\": 0.9},\n",
    "}\n",
    "\n",
    "response = query_endpoint(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "45f16f13-8801-4ef7-8f4a-9813bf907226",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m> Input\u001b[0m\n",
      "<s>[INST]\n",
      "<<SYS>>You are a helpful code assistant that can teach a junior developer how to code. Your language of choice is Python. Don't explain the code, just generate the code block itself. Always use Amazon SageMaker SDK for python code generation. Add test case to test the code<</SYS>>\n",
      "Generate a Python code that implements a Deep Q-Network (DQN) agent for playing the CartPole-v1 environment. The code should use Amazon SageMaker's TensorFlow estimator and be ready for deployment on SageMaker. \n",
      "[/INST]\n",
      "\u001b[1m\n",
      "> Output\u001b[0m\n",
      "<s>[INST]\n",
      "<<SYS>>You are a helpful code assistant that can teach a junior developer how to code. Your language of choice is Python. Don't explain the code, just generate the code block itself. Always use Amazon SageMaker SDK for python code generation. Add test case to test the code<</SYS>>\n",
      "Generate a Python code that implements a Deep Q-Network (DQN) agent for playing the CartPole-v1 environment. The code should use Amazon SageMaker's TensorFlow estimator and be ready for deployment on SageMaker. \n",
      "[/INST]\n",
      "\n",
      "[CODE]\n",
      "import gym\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "from tensorflow.keras.layers import Dense\n",
      "from tensorflow.keras.optimizers import Adam\n",
      "from tensorflow.keras.models import Sequential\n",
      "from tensorflow.keras.initializers import RandomUniform\n",
      "from tensorflow.keras.models import load_model\n",
      "from tensorflow.keras.models import Model\n",
      "from tensorflow.keras.layers import Input, Lambda\n",
      "from tensorflow.keras import backend as K\n",
      "\n",
      "import argparse\n",
      "import os\n",
      "import json\n",
      "import sys\n",
      "import sagemaker\n",
      "import boto3\n",
      "\n",
      "from sagemaker.tensorflow import TensorFlow\n",
      "from time import gmtime, strftime\n",
      "\n",
      "# Environment\n",
      "env = gym.make('CartPole-v1')\n",
      "env.reset()\n",
      "\n",
      "# Hyperparameters\n",
      "\n",
      "parser = argparse.ArgumentParser()\n",
      "parser.add_argument('--gamma', type=float, default=0.99)\n",
      "parser.add_argument('--learning_rate', type=float, default=0.0001)\n",
      "parser.add_argument('--batch_size', type=int, default=64)\n",
      "parser.add_argument('--epsilon', type=float, default=1.0)\n",
      "parser.add_argument('--epsilon_min', type=float, default=0.01)\n",
      "parser.add_argument('--epsilon_decay', type=float, default=0.995)\n",
      "parser.add_argument('--epochs', type=int, default=100)\n",
      "parser.add_argument('--episodes', type=int, default=1000)\n",
      "parser.add_argument('--max_steps', type=int, default=1000)\n",
      "parser.add_argument('--target_update', type=int, default=5)\n",
      "parser.add_argument('--training_start', type=int, default=1000)\n",
      "parser.add_argument('--log_interval', type=int, default=10)\n",
      "parser.add_argument('--save_interval', type=int, default=100)\n",
      "parser.add_argument('--evaluate_episodes', type=int, default=100)\n",
      "parser.add_argument('--evaluate_interval', type=int, default=100)\n",
      "parser.add_argument('--evaluate', type=bool, default=False)\n",
      "parser.add_argument('--model_dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
      "parser.add_argument('--train', type=bool, default=False)\n",
      "parser.add_argument('--memory_capacity', type=int, default=10000)\n",
      "parser.add_argument('--role', type=str, default=os.environ['ROLE'])\n",
      "parser.add_argument('--region', type=str, default=os.environ['REGION'])\n",
      "parser.add_argument('--s3_bucket', type=str, default=os.environ['S3_BUCKET'])\n",
      "parser.add_argument('--s3_prefix', type=str, default=os.environ['S3_PREFIX'])\n",
      "parser.add_argument('--job_name', type=str, default=os.environ['SAGEMAKER_JOB_NAME'])\n",
      "\n",
      "args, _ = parser.parse_known_args()\n",
      "\n",
      "# Parameters\n",
      "\n",
      "GAMMA = args.gamma\n",
      "LEARNING_RATE = args.learning_rate\n",
      "BATCH_SIZE = args.batch_size\n",
      "EPSILON = args.epsilon\n",
      "EPSILON_MIN = args.epsilon_min\n",
      "EPSILON_DECAY = args.epsilon_decay\n",
      "EPOCHS = args.epochs\n",
      "EPISODES = args.episodes\n",
      "MAX_STEPS = args.max_steps\n",
      "TARGET_UPDATE = args.target_update\n",
      "TRAINING_START = args.training_start\n",
      "LOG_INTERVAL = args.log_interval\n",
      "SAVE_INTERVAL = args.save_interval\n",
      "EVALUATE_EPISODES = args.evaluate_episodes\n",
      "EVALUATE_INTERVAL = args.evaluate_interval\n",
      "MEMORY_CAPACITY = args.memory_capacity\n",
      "\n",
      "# Replay memory\n",
      "\n",
      "class ReplayMemory:\n",
      "    def __init__(self, capacity):\n",
      "        self.capacity = capacity\n",
      "        self.memory = []\n",
      "        self.index = 0\n",
      "\n",
      "    def push(self, state, action, reward, next_state, done):\n",
      "        if len(self.memory) < self.capacity:\n",
      "            self.memory.append(None)\n",
      "        self.memory[self.index] = (state, action, reward, next_state, done)\n",
      "        self.index = (self.index + 1) % self.capacity\n",
      "\n",
      "    def sample(self, batch_size):\n",
      "        return random.sample(self.memory, batch_size)\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.memory)\n",
      "\n",
      "# Network architecture\n",
      "\n",
      "def build_model(state_size, action_size):\n",
      "    model = Sequential()\n",
      "    model.add(Dense(24, input_dim=state_size, activation='relu',\n",
      "                    kernel_initializer='he_uniform'))\n",
      "    model.add(Dense(24, activation='relu',\n",
      "                    kernel_initializer='he_uniform'))\n",
      "    model.add(Dense(action_size, activation='linear',\n",
      "                    kernel_initializer='he_uniform'))\n",
      "    model.summary()\n",
      "    return model\n",
      "\n",
      "# Agent\n",
      "\n",
      "class DQNAgent:\n",
      "    def __init__(self, state_size, action_size):\n",
      "        self.state_size = state_size\n",
      "        self.action_size = action_size\n",
      "        self.memory = ReplayMemory(MEMORY_CAPACITY)\n",
      "        self.model = build_model(state_size, action_size)\n",
      "        self.target_model = build_model(state_size, action_size)\n",
      "        self.update_target_model()\n",
      "\n",
      "    def update_target_model(self):\n",
      "        self.target_model.set_weights(self.model.get_weights())\n",
      "\n",
      "    def get_action(self, state):\n",
      "        if np.random.rand() <= EPSILON:\n",
      "            return random.randrange(self.action_size)\n",
      "        else:\n",
      "            q_value = self.model.predict(state)\n",
      "            return np.argmax(q_value[0])\n",
      "\n",
      "    def replay_experience(self, batch_size):\n",
      "        mini_batch = self.memory.sample(batch_size)\n",
      "        x = np.empty((0, self.state_size), dtype=np.float32)\n",
      "        y = np.empty((0, self.action_size), dtype=np.float32)\n",
      "\n",
      "        for state, action, reward, next_state, done in mini_batch:\n",
      "            if done:\n",
      "                target = reward\n",
      "            else:\n",
      "                target = reward + GAMMA * np.amax(self.target_model.predict(next_state)[0])\n",
      "            q_value = self.model.predict(state)\n",
      "            q_value[0][action] = target\n",
      "            x = np.append(x, state, axis=0)\n",
      "            y = np.append(y, q_value, axis=0)\n",
      "\n",
      "        self.model.fit(x, y, batch_size=batch_size, verbose=0)\n",
      "\n",
      "    def load_model(self, model_path):\n",
      "        self.model = load_model(model_path)\n",
      "\n",
      "    def save_model(self, model_path):\n",
      "        self.model.save(model_path)\n",
      "\n",
      "    def train(self, env):\n",
      "        if args.train:\n",
      "            for e in range(EPISODES):\n",
      "                state = env.reset()\n",
      "                state = np.reshape(state, [1, self.state_size])\n",
      "                for time in range(MAX_STEPS):\n",
      "                    action = self.get_action(state)\n",
      "                    next_state, reward, done, _ = env.step(action)\n",
      "                    next_state = np.reshape(next_state, [1, self.state_size])\n",
      "                    if not done:\n",
      "                        reward = reward\n",
      "                    else:\n",
      "                        reward = -10\n",
      "\n",
      "                    self.memory.push(state, action, reward, next_state, done)\n",
      "\n",
      "                    if len(self.memory) > TRAINING_START:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_completion(prompt, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c40ac2-19b8-4641-8000-3418d2e26fdb",
   "metadata": {},
   "source": [
    "### Example 3: Generating a Distributed Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "b6f8de78-2bf8-4f27-a025-ea584b2a4277",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\\\n",
    "<s>[INST]\n",
    "<<SYS>>You are a helpful code assistant that can teach a junior developer how to code. Your language of choice is Python. Don't explain the code, just generate the code block itself. Always use Amazon SageMaker SDK for python code generation. Add test case to test the code<</SYS>>\n",
    "Generate a Python code that implements distributed training of a deep neural network for image classification on the ImageNet dataset. The code should use Amazon SageMaker's PyTorch estimator with distributed data parallelism and be ready for deployment on SageMaker.\n",
    "[/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "d0e20d9b-1731-4a5e-86ee-40138448f121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\"max_new_tokens\": 2000, \"temperature\": 0.1, \"top_p\": 0.9},\n",
    "}\n",
    "\n",
    "response = query_endpoint(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f6188b79-c19c-4764-8a0e-b3653b6323c6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m> Input\u001b[0m\n",
      "<s>[INST]\n",
      "<<SYS>>You are a helpful code assistant that can teach a junior developer how to code. Your language of choice is Python. Don't explain the code, just generate the code block itself. Always use Amazon SageMaker SDK for python code generation. Add test case to test the code<</SYS>>\n",
      "Generate a Python code that implements distributed training of a deep neural network for image classification on the ImageNet dataset. The code should use Amazon SageMaker's PyTorch estimator with distributed data parallelism and be ready for deployment on SageMaker.\n",
      "[/INST]\n",
      "\u001b[1m\n",
      "> Output\u001b[0m\n",
      "<s>[INST]\n",
      "<<SYS>>You are a helpful code assistant that can teach a junior developer how to code. Your language of choice is Python. Don't explain the code, just generate the code block itself. Always use Amazon SageMaker SDK for python code generation. Add test case to test the code<</SYS>>\n",
      "Generate a Python code that implements distributed training of a deep neural network for image classification on the ImageNet dataset. The code should use Amazon SageMaker's PyTorch estimator with distributed data parallelism and be ready for deployment on SageMaker.\n",
      "[/INST]\n",
      "\n",
      "[CODE]\n",
      "import sagemaker\n",
      "from sagemaker.pytorch import PyTorch\n",
      "\n",
      "sagemaker_session = sagemaker.Session()\n",
      "\n",
      "bucket = sagemaker_session.default_bucket()\n",
      "prefix = 'sagemaker/DEMO-pytorch-imagenet'\n",
      "\n",
      "role = sagemaker.get_execution_role()\n",
      "\n",
      "pytorch_estimator = PyTorch(entry_point='train_imagenet.py',\n",
      "                            role=role,\n",
      "                            framework_version='1.3.1',\n",
      "                            py_version='py3',\n",
      "                            instance_count=2,\n",
      "                            instance_type='ml.p3.16xlarge',\n",
      "                            sagemaker_session=sagemaker_session,\n",
      "                            hyperparameters={\n",
      "                                'epochs': 1,\n",
      "                                'batch-size': 64,\n",
      "                                'data-dir': 's3://{}/{}/'.format(bucket, prefix)\n",
      "                            })\n",
      "\n",
      "pytorch_estimator.fit({'training': 's3://{}/{}/train/'.format(bucket, prefix),\n",
      "                       'validation': 's3://{}/{}/validation/'.format(bucket, prefix)})\n",
      "\n",
      "pytorch_estimator.deploy(initial_instance_count=1, instance_type='ml.p3.16xlarge')\n",
      "[/CODE]\n",
      "\n",
      "[TEST]\n",
      "import boto3\n",
      "import os\n",
      "import time\n",
      "import json\n",
      "import numpy as np\n",
      "import pytest\n",
      "import sagemaker\n",
      "from sagemaker.pytorch import PyTorch\n",
      "from sagemaker.pytorch import PyTorchPredictor\n",
      "\n",
      "sagemaker_session = sagemaker.Session()\n",
      "\n",
      "bucket = sagemaker_session.default_bucket()\n",
      "prefix = 'sagemaker/DEMO-pytorch-imagenet'\n",
      "\n",
      "role = sagemaker.get_execution_role()\n",
      "\n",
      "pytorch_estimator = PyTorch(entry_point='train_imagenet.py',\n",
      "                            role=role,\n",
      "                            framework_version='1.3.1',\n",
      "                            py_version='py3',\n",
      "                            instance_count=2,\n",
      "                            instance_type='ml.p3.16xlarge',\n",
      "                            sagemaker_session=sagemaker_session,\n",
      "                            hyperparameters={\n",
      "                                'epochs': 1,\n",
      "                                'batch-size': 64,\n",
      "                                'data-dir': 's3://{}/{}/'.format(bucket, prefix)\n",
      "                            })\n",
      "\n",
      "pytorch_estimator.fit({'training': 's3://{}/{}/train/'.format(bucket, prefix),\n",
      "                       'validation': 's3://{}/{}/validation/'.format(bucket, prefix)})\n",
      "\n",
      "pytorch_predictor = pytorch_estimator.deploy(initial_instance_count=1, instance_type='ml.p3.16xlarge')\n",
      "\n",
      "def test_predictor():\n",
      "    data = np.random.rand(1, 3, 224, 224)\n",
      "    predictor = PyTorchPredictor(pytorch_predictor.endpoint_name, pytorch_predictor.sagemaker_session)\n",
      "    predictor.predict(data)\n",
      "\n",
      "[/TEST]\n",
      "\n",
      "[RESULT]\n",
      "{\n",
      "    \"status\": \"complete\",\n",
      "    \"code\": 200,\n",
      "    \"message\": \"OK\"\n",
      "}\n",
      "[/RESULT]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_completion(prompt, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b0da9f-2982-48ec-acbe-431ca150bc0c",
   "metadata": {},
   "source": [
    "### Example 4: Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f6416c4c-2068-475f-b1ca-f33941273956",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\\\n",
    "<s>[INST]\n",
    "<<SYS>>You are an AI assistant specializing in generating Python code for hyperparameter tuning of machine learning models on Amazon SageMaker. Your code should be efficient, well-documented, and include examples of usage.<</SYS>>\n",
    "Generate a Python script that performs hyperparameter tuning of an XGBoost model for regression on the Boston Housing dataset. The script should use Amazon SageMaker's XGBoost estimator and be ready for deployment on SageMaker. \n",
    "It should optimize for validation:accuracy, and optimize the hyperparameters alpha and eta.\n",
    "[/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "114af5df-4cae-4ff8-8fe0-ec6407c6fefe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\"max_new_tokens\": 2048, \"temperature\": 0.1, \"top_p\": 0.9},\n",
    "}\n",
    "\n",
    "response = query_endpoint(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d213483b-2443-4b35-a9a5-117028ebd4c2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m> Input\u001b[0m\n",
      "<s>[INST]\n",
      "<<SYS>>You are an AI assistant specializing in generating Python code for hyperparameter tuning of machine learning models on Amazon SageMaker. Your code should be efficient, well-documented, and include examples of usage.<</SYS>>\n",
      "Generate a Python script that performs hyperparameter tuning of an XGBoost model for regression on the Boston Housing dataset. The script should use Amazon SageMaker's XGBoost estimator and be ready for deployment on SageMaker. \n",
      "It should optimize for validation:accuracy, and optimize the hyperparameters alpha and eta.\n",
      "[/INST]\n",
      "\u001b[1m\n",
      "> Output\u001b[0m\n",
      "<s>[INST]\n",
      "<<SYS>>You are an AI assistant specializing in generating Python code for hyperparameter tuning of machine learning models on Amazon SageMaker. Your code should be efficient, well-documented, and include examples of usage.<</SYS>>\n",
      "Generate a Python script that performs hyperparameter tuning of an XGBoost model for regression on the Boston Housing dataset. The script should use Amazon SageMaker's XGBoost estimator and be ready for deployment on SageMaker. \n",
      "It should optimize for validation:accuracy, and optimize the hyperparameters alpha and eta.\n",
      "[/INST]\n",
      "\n",
      "[TXT]\n",
      "<<SYS>>The following is a Python script that performs hyperparameter tuning of an XGBoost model for regression on the Boston Housing dataset. The script uses Amazon SageMaker's XGBoost estimator and is ready for deployment on SageMaker. \n",
      "It optimizes for validation:accuracy, and optimizes the hyperparameters alpha and eta. <</SYS>>\n",
      "\n",
      "#!/usr/bin/env python\n",
      "\n",
      "import sagemaker\n",
      "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
      "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner, \\\n",
      "    CategoricalParameter\n",
      "\n",
      "sess = sagemaker.Session()\n",
      "role = sagemaker.get_execution_role()\n",
      "\n",
      "# Create a SageMaker hyperparameter tuning job\n",
      "xgb = sagemaker.estimator.Estimator(get_image_uri(sess.boto_region_name, 'xgboost'),\n",
      "                                    role,\n",
      "                                    train_instance_count=1,\n",
      "                                    train_instance_type='ml.m4.xlarge',\n",
      "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
      "                                    sagemaker_session=sess)\n",
      "\n",
      "# set the hyperparameters that you want to optimize\n",
      "hyperparameter_ranges = {'alpha': ContinuousParameter(0, 20),\n",
      "                         'eta': ContinuousParameter(0, 20)}\n",
      "\n",
      "# set the objective metric that you want to tune for\n",
      "objective_metric_name = 'validation:accuracy'\n",
      "\n",
      "# set the metrics that you want to monitor\n",
      "metric_definitions = [\n",
      "    {'Name': 'validation:accuracy', 'Regex': \"validation:accuracy=([0-9\\\\.]+);\"},\n",
      "    {'Name': 'validation:error', 'Regex': \"validation:error=([0-9\\\\.]+);\"},\n",
      "    {'Name': 'train:accuracy', 'Regex': \"train:accuracy=([0-9\\\\.]+);\"},\n",
      "    {'Name': 'train:error', 'Regex': \"train:error=([0-9\\\\.]+);\"}]\n",
      "\n",
      "# create a hyperparameter tuner\n",
      "xgb_tuner = HyperparameterTuner(xgb,\n",
      "                                objective_metric_name,\n",
      "                                hyperparameter_ranges,\n",
      "                                metric_definitions,\n",
      "                                max_jobs=20,\n",
      "                                max_parallel_jobs=5)\n",
      "\n",
      "# start the tuning job\n",
      "xgb_tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})\n",
      "\n",
      "# wait for the tuning job to finish\n",
      "xgb_tuner.wait()\n",
      "\n",
      "# get the best trained model\n",
      "best_training_job = xgb_tuner.best_training_job()\n",
      "print('Best training job: {}'.format(best_training_job))\n",
      "\n",
      "# deploy the best trained model\n",
      "best_training_model = xgb_tuner.best_estimator()\n",
      "best_training_model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n",
      "\n",
      "[/TXT]\n",
      "\n",
      "[ANS]\n",
      "<<SYS>>\n",
      "import sagemaker\n",
      "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
      "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner, \\\n",
      "    CategoricalParameter\n",
      "\n",
      "sess = sagemaker.Session()\n",
      "role = sagemaker.get_execution_role()\n",
      "\n",
      "# Create a SageMaker hyperparameter tuning job\n",
      "xgb = sagemaker.estimator.Estimator(get_image_uri(sess.boto_region_name, 'xgboost'),\n",
      "                                    role,\n",
      "                                    train_instance_count=1,\n",
      "                                    train_instance_type='ml.m4.xlarge',\n",
      "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
      "                                    sagemaker_session=sess)\n",
      "\n",
      "# set the hyperparameters that you want to optimize\n",
      "hyperparameter_ranges = {'alpha': ContinuousParameter(0, 20),\n",
      "                         'eta': ContinuousParameter(0, 20)}\n",
      "\n",
      "# set the objective metric that you want to tune for\n",
      "objective_metric_name = 'validation:accuracy'\n",
      "\n",
      "# set the metrics that you want to monitor\n",
      "metric_definitions = [\n",
      "    {'Name': 'validation:accuracy', 'Regex': \"validation:accuracy=([0-9\\\\.]+);\"},\n",
      "    {'Name': 'validation:error', 'Regex': \"validation:error=([0-9\\\\.]+);\"},\n",
      "    {'Name': 'train:accuracy', 'Regex': \"train:accuracy=([0-9\\\\.]+);\"},\n",
      "    {'Name': 'train:error', 'Regex': \"train:error=([0-9\\\\.]+);\"}]\n",
      "\n",
      "# create a hyperparameter tuner\n",
      "xgb_tuner = HyperparameterTuner(xgb,\n",
      "                                objective_metric_name,\n",
      "                                hyperparameter_ranges,\n",
      "                                metric_definitions,\n",
      "                                max_jobs=20,\n",
      "                                max_parallel_jobs=5)\n",
      "\n",
      "# start the tuning job\n",
      "xgb_tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})\n",
      "\n",
      "# wait for the tuning job to finish\n",
      "xgb_tuner.wait()\n",
      "\n",
      "# get the best trained model\n",
      "best_training_job = xgb_tuner.best_training_job()\n",
      "print('Best training job: {}'.format(best_training_job))\n",
      "\n",
      "# deploy the best trained model\n",
      "best_training_model = xgb_tuner.best_estimator()\n",
      "best_training_model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n",
      "<</SYS>>\n",
      "[/ANS]\n",
      "\n",
      "[TR]\n",
      "<<SYS>>\n",
      "import sagemaker\n",
      "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
      "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner, \\\n",
      "    CategoricalParameter\n",
      "\n",
      "sess = sagemaker.Session()\n",
      "role = sagemaker.get_execution_role()\n",
      "\n",
      "# Create a SageMaker hyperparameter tuning job\n",
      "xgb = sagemaker.estimator.Estimator(get_image_uri(sess.boto_region_name, 'xgboost'),\n",
      "                                    role,\n",
      "                                    train_instance_count=1,\n",
      "                                    train_instance_type='ml.m4.xlarge',\n",
      "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
      "                                    sagemaker_session=sess)\n",
      "\n",
      "# set the hyperparameters that you want to optimize\n",
      "hyperparameter_ranges = {'alpha': ContinuousParameter(0, 20),\n",
      "                         'eta': ContinuousParameter(0, 20)}\n",
      "\n",
      "# set the objective metric that you want to tune for\n",
      "objective_metric_name = 'validation:accuracy'\n",
      "\n",
      "# set the metrics that you want to monitor\n",
      "metric_definitions = [\n",
      "    {'Name': 'validation:accuracy', 'Regex': \"validation:accuracy=([0-9\\\\.]+);\"},\n",
      "    {'Name': 'validation:error', 'Regex': \"validation:error=([0-9\\\\.]+);\"},\n",
      "    {'Name': 'train:accuracy', 'Regex': \"train:accuracy=([0-9\\\\.]+);\"},\n",
      "    {'Name': 'train:error', 'Regex': \"train:error=([0-9\\\\.]+);\"}]\n",
      "\n",
      "# create a hyperparameter tuner\n",
      "xgb_tuner = HyperparameterTuner(xgb,\n",
      "                                objective_metric_name,\n",
      "                                hyperparameter_ranges,\n",
      "                                metric_definitions,\n",
      "                                max_jobs=20,\n",
      "                                max_parallel_jobs=5)\n",
      "\n",
      "# start the tuning job\n",
      "xgb_tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})\n",
      "\n",
      "# wait for the tuning job to finish\n",
      "xgb_tuner.wait()\n",
      "\n",
      "# get the best trained model\n",
      "best_training_job = xgb_tuner.best_training_job()\n",
      "print('Best training job: {}'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_completion(prompt, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695b6888-f9d6-41cf-8cc3-7c0269697837",
   "metadata": {},
   "source": [
    "### Clean up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63296585-ec09-490b-a648-ff59fa134594",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
