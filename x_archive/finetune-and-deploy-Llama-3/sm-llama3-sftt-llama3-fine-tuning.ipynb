{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f4271e4",
   "metadata": {},
   "source": [
    "# Fine-Tuning LlaMA 3 using FSDP, QLORA and deploying to a SageMaker Endpoint: A Step-by-Step Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e495f494-6da2-451a-9bc3-9f04776c2c96",
   "metadata": {},
   "source": [
    "In this notebook we will walk through how to fine-tune a Llama-3 LLM on Amazon SageMaker using PyTorch FSDP and Flash Attention 2 including Q-LORA and PEFT. This notebook also explains using PEFT and merging the adapters. \n",
    "\n",
    "We will quantize the model as bf16 model. We use [Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/sft_trainer) (SFT) for fine tuning the model. We will use Anthropic/Vicuna like Chat Template with User: and Assistant: roles to fine tune the model. We will use [HuggingFaceH4/no_robots] (https://huggingface.co/datasets/HuggingFaceH4/no_robots) dataset for fine tuning the model. This is a high-quality dataset of 10,000 instructions and demonstrations created by skilled human annotators. Using [FSDP](https://pytorch.org/docs/main/fsdp.html) and [Q-Lora](https://arxiv.org/abs/2305.14314) allows us to fine tune Llama-3 models on 2x consumer GPU's. FSDP enables sharding model parameters, optimizer states and gradients across data parallel workers. Q- LORA helps reduce the memmory usage for finetuning LLM while preserving full 16-bit task performance. For fine tuning in this notebook we use ml.g5.12xlarge as a SageMaker Training Job. \n",
    "\n",
    "[Amazon SageMaker](https://aws.amazon.com/sagemaker) provides a fully managed service that enables build, train and deploy ML models at scale using tools like notebooks, debuggers, profilers, pipelines, MLOps, and more â€“ all in one integrated development environment (IDE). [SageMaker Model Training](https://aws.amazon.com/sagemaker/train/) reduces the time and cost to train and tune machine learning (ML) models at scale without the need to manage infrastructure.\n",
    "\n",
    "This notebook is inspired by Philipp Schmid Blog - https://www.philschmid.de/fsdp-qlora-llama3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fa2833-e551-43e8-8d75-3800622e5c3d",
   "metadata": {},
   "source": [
    "## Model License information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4496aad2-d040-469a-94b3-e564c98f8d16",
   "metadata": {},
   "source": [
    "In this notebook we use the Meta Llama3 model from HuggingFace. This model is a gated model within HuggingFace repository. To use this model you have to agree to the license agreement (https://llama.meta.com/llama3/license) and request access to the model before it can be used in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2860a3-68b3-435d-8d2c-753c90a24da3",
   "metadata": {},
   "source": [
    "### Install the Pre-Requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885a83fd-92b8-47ad-9128-c820fb4a6188",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers \"sagemaker>=2.190.0\" \"huggingface_hub\" \"datasets[s3]==2.18.0\" --upgrade --quiet\n",
    "!pip install boto3 s3fs \"aiobotocore==2.11.0\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6e06cf",
   "metadata": {},
   "source": [
    "Logging into the HuggingFace Hub and requesting access to the meta-llama/Meta-Llama-3-8B is required to download the model and finetune the same. Please follow the [HuggingFace User Token Documentation](https://huggingface.co/docs/hub/en/security-tokens) to request tokens to be provided in the textbox appearning below after you run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986ce221-9721-4089-8f5f-9c2d4afba925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5f5960-12b5-4310-8711-323dde090b96",
   "metadata": {},
   "source": [
    "### Setup\n",
    "We will initialize the SageMaker Session required to finetune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29cc282-7e49-4db4-ac7f-782b11558395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    " \n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    " \n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    " \n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae52c6b5-984a-4ab4-b9b1-e61feb154e49",
   "metadata": {},
   "source": [
    "### Define the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b4d604-1fb4-4268-9b0a-6fae07ca53cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "# save train_dataset to s3 using our SageMaker session\n",
    "training_input_path = f's3://{sess.default_bucket()}/datasets/huggingface-h4-no-robots'\n",
    "use_bf16 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ef6d88-133b-46cc-b949-439dc06aa278",
   "metadata": {},
   "source": [
    "### Dataset Prepare\n",
    "We will use [HuggingFaceH4/no_robots](https://huggingface.co/datasets/HuggingFaceH4/no_robots) dataset to finetune the Llama 3 model. Kindly refer to the [Licensing Information](https://huggingface.co/datasets/HuggingFaceH4/no_robots#licensing-information) regarding this dataset before proceeding further.\n",
    "\n",
    "We will transform the messages to OAI format and split the data into Train and Test set. The Train and Test dataset will be uploaded into S3 - SageMaker Session Bucket for use during finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de024deb-c757-49c9-a734-bc31bdc5985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    " \n",
    "# Convert dataset to OAI messages\n",
    "system_message = \"\"\"You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\"\"\"\n",
    " \n",
    "def create_conversation(sample):\n",
    "    if sample[\"messages\"][0][\"role\"] == \"system\":\n",
    "        return sample\n",
    "    else:\n",
    "      sample[\"messages\"] = [{\"role\": \"system\", \"content\": system_message}] + sample[\"messages\"]\n",
    "      return sample\n",
    " \n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"HuggingFaceH4/no_robots\")\n",
    " \n",
    "# Add system message to each conversation\n",
    "columns_to_remove = list(dataset[\"train\"].features)\n",
    "columns_to_remove.remove(\"messages\")\n",
    "dataset = dataset.map(create_conversation, remove_columns=columns_to_remove, batched=False)\n",
    " \n",
    "# Filter out conversations which are corrupted with wrong turns, keep which have even number of turns after adding system message\n",
    "dataset[\"train\"] = dataset[\"train\"].filter(lambda x: len(x[\"messages\"][1:]) % 2 == 0)\n",
    "dataset[\"test\"] = dataset[\"test\"].filter(lambda x: len(x[\"messages\"][1:]) % 2 == 0)\n",
    " \n",
    "# save datasets to s3\n",
    "dataset[\"train\"].to_json(f\"{training_input_path}/train_dataset.json\", orient=\"records\", force_ascii=False)\n",
    "dataset[\"test\"].to_json(f\"{training_input_path}/test_dataset.json\", orient=\"records\", force_ascii=False)\n",
    " \n",
    "print(f\"Training data uploaded to:\")\n",
    "print(f\"{training_input_path}/train_dataset.json\")\n",
    "print(f\"https://s3.console.aws.amazon.com/s3/buckets/{sess.default_bucket()}/?region={sess.boto_region_name}&prefix={training_input_path.split('/', 3)[-1]}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67418b39-254d-42d8-907b-4acc92593ae4",
   "metadata": {},
   "source": [
    "### Training script and dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544aa17e-6551-439b-b519-d5531528dec2",
   "metadata": {},
   "source": [
    "Create the scripts directory to hold the training script and dependencies list. This directory will be provided to the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c48a48-9890-4404-a3dd-688e77ee7c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"scripts/trl\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4276101-a612-4a65-8a2c-e58e759f8c06",
   "metadata": {},
   "source": [
    "Create the requirements file that will be used by the SageMaker Job container to initialize the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641c9c19-c920-4279-8508-9abc01ac89f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/trl/requirements.txt\n",
    "torch==2.2.2\n",
    "transformers==4.40.2\n",
    "sagemaker>=2.190.0\n",
    "datasets==2.18.0\n",
    "accelerate==0.29.3\n",
    "evaluate==0.4.1\n",
    "bitsandbytes==0.43.1\n",
    "trl==0.8.6\n",
    "peft==0.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4533b519-3926-4f5f-b1f2-9de267d22195",
   "metadata": {},
   "source": [
    "Training Script that will use PyTorch FSDP, QLORA, PEFT and train the model using SFT Trainer. This script also includes prepping the data to Llama 3 chat template (Anthropic/Vicuna format). This training script is being written to the scripts folder along with the requirements file that will be used by the SageMaker Job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32c5f36-833d-4d45-9138-63711a805b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/trl/run_fsdp_qlora.py\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "import os\n",
    "\n",
    "try:\n",
    "    os.system(\"pip install flash-attn --no-build-isolation --upgrade\")\n",
    "except:\n",
    "    print(\"flash-attn failed to install\")\n",
    "\n",
    "import random\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from trl.commands.cli_utils import  TrlParser\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    HfArgumentParser,\n",
    "    BitsAndBytesConfig,\n",
    "        set_seed,\n",
    "\n",
    ")\n",
    "from trl import setup_chat_format\n",
    "from peft import LoraConfig\n",
    "\n",
    "\n",
    "from trl import (SFTTrainer)\n",
    "\n",
    "\n",
    "# Comment in if you want to use the Llama 3 instruct template but make sure to add modules_to_save\n",
    "# LLAMA_3_CHAT_TEMPLATE=\"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"\n",
    "\n",
    "# Anthropic/Vicuna like template without the need for special tokens\n",
    "LLAMA_3_CHAT_TEMPLATE = (\n",
    "    \"{% for message in messages %}\"\n",
    "        \"{% if message['role'] == 'system' %}\"\n",
    "            \"{{ message['content'] }}\"\n",
    "        \"{% elif message['role'] == 'user' %}\"\n",
    "            \"{{ '\\n\\nHuman: ' + message['content'] +  eos_token }}\"\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\n",
    "            \"{{ '\\n\\nAssistant: '  + message['content'] +  eos_token  }}\"\n",
    "        \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if add_generation_prompt %}\"\n",
    "    \"{{ '\\n\\nAssistant: ' }}\"\n",
    "    \"{% endif %}\"\n",
    ")\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    dataset_path: str = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Path to the dataset\"\n",
    "        },\n",
    "    )\n",
    "    model_id: str = field(\n",
    "        default=None, metadata={\"help\": \"Model ID to use for SFT training\"}\n",
    "    )\n",
    "    max_seq_length: int = field(\n",
    "        default=512, metadata={\"help\": \"The maximum sequence length for SFT Trainer\"}\n",
    "    )\n",
    "    use_qlora: bool = field(default=False, metadata={\"help\": \"Whether to use QLORA\"})\n",
    "    merge_adapters: bool = field(\n",
    "        metadata={\"help\": \"Wether to merge weights for LoRA.\"},\n",
    "        default=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def training_function(script_args, training_args):\n",
    "    ################\n",
    "    # Dataset\n",
    "    ################\n",
    "    \n",
    "    train_dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=os.path.join(script_args.dataset_path, \"train_dataset.json\"),\n",
    "        split=\"train\",\n",
    "    )\n",
    "    test_dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=os.path.join(script_args.dataset_path, \"test_dataset.json\"),\n",
    "        split=\"train\",\n",
    "    )\n",
    "\n",
    "    ################\n",
    "    # Model & Tokenizer\n",
    "    ################\n",
    "\n",
    "    # Tokenizer        \n",
    "    tokenizer = AutoTokenizer.from_pretrained(script_args.model_id, use_fast=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.chat_template = LLAMA_3_CHAT_TEMPLATE\n",
    "    \n",
    "    # template dataset\n",
    "    def template_dataset(examples):\n",
    "        return{\"text\":  tokenizer.apply_chat_template(examples[\"messages\"], tokenize=False)}\n",
    "    \n",
    "    train_dataset = train_dataset.map(template_dataset, remove_columns=[\"messages\"])\n",
    "    test_dataset = test_dataset.map(template_dataset, remove_columns=[\"messages\"])\n",
    "    \n",
    "    # print random sample\n",
    "    with training_args.main_process_first(\n",
    "        desc=\"Log a few random samples from the processed training set\"\n",
    "    ):\n",
    "        for index in random.sample(range(len(train_dataset)), 2):\n",
    "            print(train_dataset[index][\"text\"])\n",
    "\n",
    "    # Model    \n",
    "    torch_dtype = torch.bfloat16 if training_args.bf16 else torch.float32\n",
    "    quant_storage_dtype = torch.bfloat16\n",
    "\n",
    "    if script_args.use_qlora:\n",
    "        print(f\"Using QLoRA - {torch_dtype}\")\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch_dtype,\n",
    "                bnb_4bit_quant_storage=quant_storage_dtype,\n",
    "            )\n",
    "    else:\n",
    "        quantization_config = None\n",
    "        \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        script_args.model_id,\n",
    "        quantization_config=quantization_config,\n",
    "        #device_map=\"auto\",\n",
    "        device_map={'':torch.cuda.current_device()},\n",
    "        attn_implementation=\"sdpa\", # use sdpa, alternatively use \"flash_attention_2\"\n",
    "        torch_dtype=quant_storage_dtype,\n",
    "        use_cache=False if training_args.gradient_checkpointing else True,  # this is needed for gradient checkpointing\n",
    "    )\n",
    "    \n",
    "    if training_args.gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "    ################\n",
    "    # PEFT\n",
    "    ################\n",
    "\n",
    "    # LoRA config based on QLoRA paper & Sebastian Raschka experiment\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=8,\n",
    "        lora_dropout=0.05,\n",
    "        r=16,\n",
    "        bias=\"none\",\n",
    "        target_modules=\"all-linear\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        # modules_to_save = [\"lm_head\", \"embed_tokens\"] # add if you want to use the Llama 3 instruct template\n",
    "    )\n",
    "\n",
    "    ################\n",
    "    # Training\n",
    "    ################\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        eval_dataset=test_dataset,\n",
    "        peft_config=peft_config,\n",
    "        max_seq_length=script_args.max_seq_length,\n",
    "        tokenizer=tokenizer,\n",
    "        packing=True,\n",
    "        dataset_kwargs={\n",
    "            \"add_special_tokens\": False,  # We template with special tokens\n",
    "            \"append_concat_token\": False,  # No need to add additional separator token\n",
    "        },\n",
    "    )\n",
    "    if trainer.accelerator.is_main_process:\n",
    "        trainer.model.print_trainable_parameters()\n",
    "\n",
    "    ##########################\n",
    "    # Train model\n",
    "    ##########################\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "    trainer.train(resume_from_checkpoint=checkpoint)\n",
    "\n",
    "    ##########################\n",
    "    # SAVE MODEL FOR SAGEMAKER\n",
    "    ##########################\n",
    "    sagemaker_save_dir = \"/opt/ml/model\"\n",
    "\n",
    "    if trainer.is_fsdp_enabled:\n",
    "        trainer.accelerator.state.fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")\n",
    "    trainer.save_model(sagemaker_save_dir)\n",
    "\n",
    "    if script_args.merge_adapters:\n",
    "        # merge adapter weights with base model and save\n",
    "        # save int 4 model\n",
    "        print('########## Merging Adapters  ##########')\n",
    "        trainer.model.save_pretrained(training_args.output_dir)\n",
    "        trainer.tokenizer.save_pretrained(training_args.output_dir)\n",
    "        # clear memory\n",
    "        del model\n",
    "        del trainer\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "        # list file in output_dir\n",
    "        print(os.listdir(training_args.output_dir))\n",
    "\n",
    "        # load PEFT model in fp16\n",
    "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            training_args.output_dir,\n",
    "            low_cpu_mem_usage=True,\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        # Merge LoRA and base model and save\n",
    "        model = model.merge_and_unload()\n",
    "        model.save_pretrained(\n",
    "            sagemaker_save_dir, safe_serialization=True, max_shard_size=\"2GB\"\n",
    "        )\n",
    "    else:\n",
    "        trainer.model.save_pretrained(sagemaker_save_dir, safe_serialization=True)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    parser = HfArgumentParser((ScriptArguments, TrainingArguments))\n",
    "    script_args, training_args = parser.parse_args_into_dataclasses()    \n",
    "    \n",
    "    # set use reentrant to False\n",
    "    if training_args.gradient_checkpointing:\n",
    "        training_args.gradient_checkpointing_kwargs = {\"use_reentrant\": True}\n",
    "    # set seed\n",
    "    set_seed(training_args.seed)\n",
    "  \n",
    "    # launch training\n",
    "    training_function(script_args, training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c74d7a-9045-4e24-a0b1-6154f5ff66e1",
   "metadata": {},
   "source": [
    "Hyperparameters, which are passed into the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1bbfe3-c4b2-478e-90b8-68a657109c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "  ### SCRIPT PARAMETERS ###\n",
    "  'dataset_path': '/opt/ml/input/data/training/',    # path where sagemaker will save training dataset\n",
    "  'model_id': model_id,                              # or `mistralai/Mistral-7B-v0.1`\n",
    "  'max_seq_len': 3072,                               # max sequence length for model and packing of the dataset\n",
    "  'use_qlora': True,                                 # use QLoRA model\n",
    "  ### TRAINING PARAMETERS ###\n",
    "  'num_train_epochs': 1,                             # number of training epochs\n",
    "  'per_device_train_batch_size': 1,                  # batch size per device during training\n",
    "  'per_device_eval_batch_size': 1,                   # batch size for evaluation    \n",
    "  'gradient_accumulation_steps': 4,                  # number of steps before performing a backward/update pass\n",
    "  'gradient_checkpointing': True,                    # use gradient checkpointing to save memory\n",
    "  'optim': \"adamw_torch\",                            # use fused adamw optimizer\n",
    "  'logging_steps': 10,                               # log every 10 steps\n",
    "  'save_strategy': \"epoch\",                          # save checkpoint every epoch\n",
    "  'evaluation_strategy': \"epoch\",\n",
    "  'learning_rate': 0.0002,                           # learning rate, based on QLoRA paper\n",
    "  'bf16': use_bf16,                                  # use bfloat16 precision\n",
    "  'tf32': True,                                      # use tf32 precision\n",
    "  'max_grad_norm': 0.3,                              # max gradient norm based on QLoRA paper\n",
    "  'warmup_ratio': 0.03,                              # warmup ratio based on QLoRA paper\n",
    "  'lr_scheduler_type': \"constant\",                   # use constant learning rate scheduler\n",
    "  'report_to': \"tensorboard\",                        # report metrics to tensorboard\n",
    "  'output_dir': '/tmp/tun',                          # Temporary output directory for model checkpoints\n",
    "  'merge_adapters': True,                            # merge LoRA adapters into model for easier deployment\n",
    "  'fsdp': '\"full_shard auto_wrap offload\"',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a292a6b3-3123-451b-ad2c-cf6fda7edc54",
   "metadata": {},
   "source": [
    "Use the SageMaker HuggingFace Estimator to finetune the model passing in the hyperparameters and the scripts directory from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd877c88-812b-4c53-b6c2-2c7fb27a6a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder \n",
    "import time\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f'{model_id.replace(\"/\", \"-\")}-{\"bf16\" if use_bf16 else \"f32\" }-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    " \n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_fsdp_qlora.py',    # train script\n",
    "    source_dir           = 'scripts/trl/',      # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.12xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    max_run              = 2*24*60*60,        # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.36.0',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.1.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    disable_output_compression = False,        # not compress output to save training time and cost\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},\n",
    "    environment          = {\n",
    "        \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\", # set env variable to cache models in /tmp\n",
    "        \"HF_TOKEN\": HfFolder.get_token(),       # Retrieve HuggingFace Token to be used for downloading base models from\n",
    "        \"ACCELERATE_USE_FSDP\":\"1\", \n",
    "        \"FSDP_CPU_RAM_EFFICIENT_LOADING\":\"1\"\n",
    "    },\n",
    ")\n",
    "\n",
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    " \n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a05e8a-0317-4f99-8181-f859a9022656",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755a7486-60cc-4cb5-9a01-0f4c25d9f331",
   "metadata": {},
   "source": [
    "## Deploy the fine tuned model as SageMaker Endpoint and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3827b1de-8deb-45e5-b982-df3d287a8c61",
   "metadata": {},
   "source": [
    "Use a Huggingface container image that is not yet released in the SageMaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ab6f97-767b-42b4-9b07-4ecfdd93bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_image = f\"763104351884.dkr.ecr.{sess.boto_region_name}.amazonaws.com/huggingface-pytorch-tgi-inference:2.1-tgi2.0-gpu-py310-cu121-ubuntu22.04\"\n",
    " \n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f889ce3-0663-4b51-8214-f6b8564f24a1",
   "metadata": {},
   "source": [
    "Now lets deploy the Finetuned model using the container above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b57938-1ad3-4728-b777-912012e29fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import json\n",
    "\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "health_check_timeout = 900\n",
    "number_of_gpu = 4\n",
    "\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"/opt/ml/model\", # path to where sagemaker stores the model\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(8000),  # Max length of input text\n",
    "  'MAX_BATCH_PREFILL_TOKENS': json.dumps(16384),  # Number of tokens for the prefill operation.\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(16384),  # Max length of the generation (including input text)\n",
    "}\n",
    "\n",
    "huggingface_llama_model = HuggingFaceModel(\n",
    "    model_data = huggingface_estimator.model_data,\n",
    "    role = role, \n",
    "    image_uri= llm_image,\n",
    "    env=config\n",
    ")\n",
    "\n",
    "predictor = huggingface_llama_model.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type=instance_type, \n",
    "    container_startup_health_check_timeout=health_check_timeout, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c00eac-845f-4cf9-8d32-c5f77735f2fa",
   "metadata": {},
   "source": [
    "Utility function to format print the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257d4797-1f54-4f6b-bf66-eac97e35958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_response(payload, response):\n",
    "    print(payload[\"inputs\"])\n",
    "    print(f\"> {response[0]['generated_text']}\")\n",
    "    print(\"\\n==================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ac8548-fe53-473d-a1e8-ad218ded987e",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c30094c-7f0d-44fb-ae37-58bf31c99066",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": \"I believe the meaning of life is\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 64,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "        \"return_full_text\": False,\n",
    "    },\n",
    "}\n",
    "try:\n",
    "    response = predictor.predict(payload, custom_attributes=\"accept_eula=false\")\n",
    "    print_response(payload, response)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c26142b-758b-42df-96b0-8eb76dbd4ba0",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0c2f4b-d746-4859-84aa-166ccff4b9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": \"Simply put, the theory of relativity states that \",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 64,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "        \"return_full_text\": False,\n",
    "    },\n",
    "}\n",
    "try:\n",
    "    response = predictor.predict(payload, custom_attributes=\"accept_eula=false\")\n",
    "    print_response(payload, response)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cb7f11-d9cc-45ee-a2ca-c893516ff383",
   "metadata": {},
   "source": [
    "### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456e4a86-381d-44a9-b615-e1bac6f80a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": \"\"\"A brief message congratulating the team on the launch:\n",
    "\n",
    "Hi everyone,\n",
    "\n",
    "I just \"\"\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 64,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "        \"return_full_text\": False,\n",
    "    },\n",
    "}\n",
    "try:\n",
    "    response = predictor.predict(payload, custom_attributes=\"accept_eula=false\")\n",
    "    print_response(payload, response)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f432a5b-a7d2-4ea5-80ee-eb264b9749ee",
   "metadata": {},
   "source": [
    "### Example 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f13ad6-ae0f-4896-aab1-41e0abf9f5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": \"\"\"Translate English to French:\n",
    "sea otter => loutre de mer\n",
    "peppermint => menthe poivrÃ©e\n",
    "plush girafe => girafe peluche\n",
    "cheese =>\"\"\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 64,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "        \"return_full_text\": False,\n",
    "    },\n",
    "}\n",
    "try:\n",
    "    response = predictor.predict(payload, custom_attributes=\"accept_eula=false\")\n",
    "    print_response(payload, response)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7068422e-4f96-4057-85cd-31bfbd5e5867",
   "metadata": {},
   "source": [
    "### Cleanup both the model and the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3f69b9-947c-4073-a122-24d6cf29efa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
